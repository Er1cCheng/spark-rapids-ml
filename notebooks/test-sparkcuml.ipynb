{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4319ad5-7b8d-47ae-8227-230ce6ee40ec",
   "metadata": {},
   "source": [
    "# Pyspark Compatibility Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b659d5-5e0b-422a-a653-cd99502db80c",
   "metadata": {},
   "source": [
    "For testing against revision just before renaming package: [3bb8c8b64d0d10c720882f8a2110c20eaff23907](https://gitlab-master.nvidia.com/nvspark/spark-cuml/-/tree/3bb8c8b64d0d10c720882f8a2110c20eaff23907)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df866f0-8ca1-458a-a427-f01ffcdd77ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PCA\n",
    "From: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.PCA.html#pyspark.ml.feature.PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d73ca9e5-d033-435b-8f8c-2aa64ee2a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYSPARK = False\n",
    "SPARK_RAPIDS_ML = not PYSPARK\n",
    "\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65bb00c7-7aba-45a6-8998-3182051b2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    from pyspark.ml.feature import PCA, PCAModel\n",
    "else:\n",
    "    from sparkcuml.feature import PCA, PCAModel\n",
    "\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f56d981-48b1-4daf-acb6-8ba4fdba06a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 1.0, 0.0, 7.0, 0.0],\n",
       " [2.0, 0.0, 3.0, 4.0, 5.0],\n",
       " [4.0, 0.0, 0.0, 6.0, 7.0]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if PYSPARK:\n",
    "    data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "            (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "            (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "else:\n",
    "    # # Sparse vectors not supported in spark-rapids-ml\n",
    "    # data = [(Vectors.dense([0.0, 1.0, 0.0, 7.0, 0.0]),),\n",
    "    #         (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "    #         (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "\n",
    "    # Vectors not supported in this revision of code\n",
    "    data = [[0.0, 1.0, 0.0, 7.0, 0.0],\n",
    "            [2.0, 0.0, 3.0, 4.0, 5.0],\n",
    "            [4.0, 0.0, 0.0, 6.0, 7.0]]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae01021-9ccd-4545-a054-f61c39c6b224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[0.0, 1.0, 0.0, 7...|\n",
      "|[2.0, 0.0, 3.0, 4...|\n",
      "|[4.0, 0.0, 0.0, 6...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('features', ArrayType(DoubleType(), True), True)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if PYSPARK:\n",
    "    df = spark.createDataFrame(data, [\"features\"])\n",
    "else:\n",
    "    tmp = pd.DataFrame(data)\n",
    "    pdf = pd.DataFrame()\n",
    "    pdf[\"features\"] = tmp.values.tolist()\n",
    "    df = spark.createDataFrame(pdf)\n",
    "\n",
    "df.show(); df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f98d73-95b2-4e23-b745-92b4a9aac47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    pca = PCA(k=2, inputCol=\"features\")\n",
    "else:\n",
    "    # pca = PCA(k=2, inputCol=\"features\")    # ValueError: Unsupported param 'k'.\n",
    "    pca = PCA(n_components=2, inputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a74871fd-c360-42fd-82ea-3e129f44b012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA_60986327fc73"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.setOutputCol(\"pca_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966803e8-8a96-4f0f-a046-0eecdd0c0769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputCol: input column name. (current: features)\n",
      "inputCols: input column names. (undefined)\n",
      "n_components: Refer to CUML doc of cuml.decomposition.pca.PCA for this param n_components (default: None, current: 2)\n",
      "num_workers: The number of Spark CUML workers. Each CUML worker corresponds to one spark task. (default: 1)\n",
      "outputCol: output column name. (default: PCA_60986327fc73__output, current: pca_features)\n",
      "outputCols: output column names. (undefined)\n",
      "svd_solver: Refer to CUML doc of cuml.decomposition.pca.PCA for this param svd_solver (default: auto)\n",
      "verbose: Refer to CUML doc of cuml.decomposition.pca.PCA for this param verbose (default: False)\n",
      "whiten: Refer to CUML doc of cuml.decomposition.pca.PCA for this param whiten (default: False)\n"
     ]
    }
   ],
   "source": [
    "# Note: cuML parameters exposed as Spark ML Params\n",
    "print(pca.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a08f6cff-2b8e-4f4f-9d36-a3e2321d9ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputCol: input column name. (current: features)\n",
      "inputCols: input column names. (undefined)\n",
      "n_components: Refer to CUML doc of cuml.decomposition.pca.PCA for this param n_components (default: None, current: 3)\n",
      "num_workers: The number of Spark CUML workers. Each CUML worker corresponds to one spark task. (default: 1)\n",
      "outputCol: output column name. (default: PCA_60986327fc73__output, current: pca_features)\n",
      "outputCols: output column names. (undefined)\n",
      "svd_solver: Refer to CUML doc of cuml.decomposition.pca.PCA for this param svd_solver (default: auto)\n",
      "verbose: Refer to CUML doc of cuml.decomposition.pca.PCA for this param verbose (default: False)\n",
      "whiten: Refer to CUML doc of cuml.decomposition.pca.PCA for this param whiten (default: False)\n"
     ]
    }
   ],
   "source": [
    "pca.setK(3)\n",
    "print(pca.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95fcc604-10af-42af-800f-caa322730c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputCol: input column name. (current: features)\n",
      "inputCols: input column names. (undefined)\n",
      "n_components: Refer to CUML doc of cuml.decomposition.pca.PCA for this param n_components (default: None, current: 2)\n",
      "num_workers: The number of Spark CUML workers. Each CUML worker corresponds to one spark task. (default: 1)\n",
      "outputCol: output column name. (default: PCA_60986327fc73__output, current: pca_features)\n",
      "outputCols: output column names. (undefined)\n",
      "svd_solver: Refer to CUML doc of cuml.decomposition.pca.PCA for this param svd_solver (default: auto)\n",
      "verbose: Refer to CUML doc of cuml.decomposition.pca.PCA for this param verbose (default: False)\n",
      "whiten: Refer to CUML doc of cuml.decomposition.pca.PCA for this param whiten (default: False)\n"
     ]
    }
   ],
   "source": [
    "pca.setK(2)\n",
    "print(pca.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a61234f5-070c-4fb4-ad2f-c0b741cf79ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = pca.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3e74f81-476d-4423-bbe5-46b59480d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    # Note: no getter for 'n_components'\n",
    "    model.getK()\n",
    "    # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a70f79c-2f66-4a5e-82a9-c2cfe02f490e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCAModel_64bd8de451d8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.setOutputCol(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11ed38a9-743e-4598-9ccf-dc28b2e78ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputCol: input column name. (current: features)\n",
      "inputCols: input column names. (undefined)\n",
      "n_components: Refer to CUML doc of cuml.decomposition.pca.PCA for this param n_components (default: None, current: 2)\n",
      "outputCol: output column name. (default: PCA_60986327fc73__output, current: output)\n",
      "outputCols: output column names. (undefined)\n",
      "svd_solver: Refer to CUML doc of cuml.decomposition.pca.PCA for this param svd_solver (default: auto, current: auto)\n",
      "verbose: Refer to CUML doc of cuml.decomposition.pca.PCA for this param verbose (default: False, current: False)\n",
      "whiten: Refer to CUML doc of cuml.decomposition.pca.PCA for this param whiten (default: False, current: False)\n"
     ]
    }
   ],
   "source": [
    "print(model.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08e8b235-e7a3-4694-9a62-46e9ec93094d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-4.790376837878261, -0.5239389022984442]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(df).collect()[0].output\n",
    "# DenseVector([1.648..., -4.013...])\n",
    "# Note: result is different because cuML does implied mean normalization, where Spark doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c1db46f-72f8-43f6-aaf8-6aebd4c7318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18.00624707305587, 4.660419593610799]\n"
     ]
    }
   ],
   "source": [
    "if PYSPARK:\n",
    "    print(model.explainedVariance)\n",
    "else:\n",
    "    print(model.explained_variance)\n",
    "\n",
    "# DenseVector([0.794..., 0.205...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99f3e554-fb17-44f5-bad2-ae8b306167ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.4485917207506905,\n",
       "  -0.133019857453954,\n",
       "  0.12523156359767595,\n",
       "  -0.21650756651938066,\n",
       "  0.847651293112682],\n",
       " [-0.28423808042763415,\n",
       "  -0.056211552389868726,\n",
       "  0.7636264781646386,\n",
       "  -0.565295877910818,\n",
       "  -0.11560340512131573]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pc\n",
    "# DenseMatrix(5, 2, [-0.4486, 0.133, -0.1252, 0.2165, -0.8477, -0.2842, -0.0562, 0.7636, -0.5653, -0.1156], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8da1934e-cd40-40d5-8f56-7d711ed06620",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/tmp\"\n",
    "pcaPath = temp_path + \"/pca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e55bfaa3-a0c4-4f54-9893-79f8f82410bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(pcaPath, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "deea9119-6f4d-4233-8e6a-28e5ccee6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.save(pcaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cd390f1-c709-4f4d-9a83-f86b1e0573e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadedPca = PCA.load(pcaPath)\n",
    "\n",
    "if PYSPARK:\n",
    "    loadedPca.getK() == pca.getK()\n",
    "    # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02a6c8a1-d5d8-40af-9804-d600e503bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = temp_path + \"/pca-model\"\n",
    "shutil.rmtree(modelPath, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "353986ab-fba7-48a7-9f97-2db5ce1490ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47aad8bd-b4bb-410d-873d-bdc9ba9d0c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedModel = PCAModel.load(modelPath)\n",
    "loadedModel.pc == model.pc\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44681be2-a2a5-4111-8fb0-46a77dfa61dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if PYSPARK:\n",
    "    print(loadedModel.explainedVariance == model.explainedVariance)\n",
    "    # True\n",
    "else:\n",
    "    print(loadedModel.explained_variance == model.explained_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "081bb7ef-86b6-45e3-88d8-cea892777bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedModel.transform(df).take(1) == model.transform(df).take(1)\n",
    "# True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f3f77-2016-43ae-8b8c-c8fd2b7117e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## KMeans\n",
    "From: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.KMeans.html#pyspark.ml.clustering.KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cc1d3be-9835-4157-b836-a49f4a6d0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "else:\n",
    "    from sparkcuml.clustering import KMeans, KMeansModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5f09d6f-66cd-4b1e-9926-b9fe0eb08119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1b3d2b2-91da-4ec5-abbe-19667ac0f992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|  features|weighCol|\n",
      "+----------+--------+\n",
      "|[0.0, 0.0]|     2.0|\n",
      "|[1.0, 1.0]|     2.0|\n",
      "|[9.0, 8.0]|     2.0|\n",
      "|[8.0, 9.0]|     2.0|\n",
      "+----------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('features', ArrayType(DoubleType(), True), True), StructField('weighCol', DoubleType(), True)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if PYSPARK:\n",
    "    data = [(Vectors.dense([0.0, 0.0]), 2.0), (Vectors.dense([1.0, 1.0]), 2.0),\n",
    "            (Vectors.dense([9.0, 8.0]), 2.0), (Vectors.dense([8.0, 9.0]), 2.0)]\n",
    "    df = spark.createDataFrame(data, [\"features\", \"weighCol\"])\n",
    "else:\n",
    "    # Vectors not supported in this revision of code\n",
    "    data = [([0.0, 0.0], 2.0),\n",
    "            ([1.0, 1.0], 2.0),\n",
    "            ([9.0, 8.0], 2.0),\n",
    "            ([8.0, 9.0], 2.0)]\n",
    "    pdf = pd.DataFrame(data)\n",
    "    df = spark.createDataFrame(pdf, [\"features\", \"weighCol\"]).repartition(1)\n",
    "\n",
    "df.show(); df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "238bde84-42a4-4280-9f7e-c0fc678128e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    kmeans = KMeans(k=2)\n",
    "    kmeans.setSeed(1)\n",
    "    kmeans.setMaxIter(10)\n",
    "    kmeans.setWeightCol(\"weighCol\")\n",
    "else:\n",
    "    kmeans = KMeans(n_clusters=2, random_state=1, max_iter=10, inputCol=\"features\", outputCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c658398a-0285-4438-9abe-0ccd196f625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param init (default: scalable-k-means++)\n",
      "inputCol: input column name. (current: features)\n",
      "inputCols: input column names. (undefined)\n",
      "max_iter: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param max_iter (default: 300, current: 10)\n",
      "max_samples_per_batch: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param max_samples_per_batch (default: 32768)\n",
      "n_clusters: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param n_clusters (default: 8, current: 2)\n",
      "n_init: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param n_init (default: 1)\n",
      "num_workers: The number of Spark CUML workers. Each CUML worker corresponds to one spark task. (default: 1)\n",
      "outputCol: output column name. (default: KMeans_4d99fa7d922c__output, current: prediction)\n",
      "outputCols: output column names. (undefined)\n",
      "oversampling_factor: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param oversampling_factor (default: 2.0)\n",
      "random_state: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param random_state (default: 1, current: 1)\n",
      "tol: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param tol (default: 0.0001)\n",
      "verbose: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param verbose (default: False)\n"
     ]
    }
   ],
   "source": [
    "print(kmeans.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94b3278f-d5ee-4680-89be-b849b9991a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    kmeans.getMaxIter()\n",
    "else:\n",
    "    # AttributeError: 'KMeans' object has no attribute 'getMaxIter'\n",
    "    pass\n",
    "\n",
    "# 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f137bb82-8bd3-4909-9df5-8be5a2d2fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    kmeans.clear(kmeans.maxIter)\n",
    "else:\n",
    "    # AttributeError: 'KMeans' object has no attribute 'maxIter'\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c155513a-4386-4f08-b8f6-0b9ef2c46947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param init (default: scalable-k-means++)\n",
      "inputCol: input column name. (current: features)\n",
      "inputCols: input column names. (undefined)\n",
      "max_iter: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param max_iter (default: 300, current: 10)\n",
      "max_samples_per_batch: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param max_samples_per_batch (default: 32768)\n",
      "n_clusters: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param n_clusters (default: 8, current: 2)\n",
      "n_init: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param n_init (default: 1)\n",
      "num_workers: The number of Spark CUML workers. Each CUML worker corresponds to one spark task. (default: 1)\n",
      "outputCol: output column name. (default: KMeans_4d99fa7d922c__output, current: prediction)\n",
      "outputCols: output column names. (undefined)\n",
      "oversampling_factor: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param oversampling_factor (default: 2.0)\n",
      "random_state: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param random_state (default: 1, current: 1)\n",
      "tol: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param tol (default: 0.0001)\n",
      "verbose: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param verbose (default: False)\n"
     ]
    }
   ],
   "source": [
    "print(kmeans.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0aa3a31-c619-4201-81c7-09bbf566e2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = kmeans.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8e06e0c-302f-441b-9d74-de0b1b780094",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    model.getDistanceMeasure()\n",
    "    # 'euclidean'\n",
    "else:\n",
    "    # AttributeError: 'KMeansModel' object has no attribute 'getDistanceMeasure'\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3cc0c6a-a09a-4da7-b037-b4101244b780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeansModel_acb1db2605b1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.setPredictionCol(\"newPrediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3915c572-5250-4005-857b-d3beacf03d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param init (default: scalable-k-means++, current: scalable-k-means++)\n",
      "inputCol: input column name. (current: features)\n",
      "inputCols: input column names. (undefined)\n",
      "max_iter: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param max_iter (default: 300, current: 10)\n",
      "max_samples_per_batch: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param max_samples_per_batch (default: 32768, current: 32768)\n",
      "n_clusters: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param n_clusters (default: 8, current: 2)\n",
      "n_init: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param n_init (default: 1, current: 1)\n",
      "outputCol: output column name. (default: KMeans_4d99fa7d922c__output, current: newPrediction)\n",
      "outputCols: output column names. (undefined)\n",
      "oversampling_factor: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param oversampling_factor (default: 2.0, current: 2.0)\n",
      "random_state: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param random_state (default: 1, current: 1)\n",
      "tol: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param tol (default: 0.0001, current: 0.0001)\n",
      "verbose: Refer to CUML doc of cuml.cluster.kmeans.KMeans for this param verbose (default: False, current: False)\n"
     ]
    }
   ],
   "source": [
    "print(model.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0a23fe30-d6c3-4e60-b444-9943ca0efdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    model.predict(df.head().features)\n",
    "    # 0\n",
    "else:\n",
    "    # AttributeError: 'KMeansModel' object has no attribute 'predict'\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51d7bf2a-7fea-4e06-bea1-b9c28d3fa348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if PYSPARK:\n",
    "    centers = model.clusterCenters()\n",
    "else:\n",
    "    centers = model.cluster_centers_\n",
    "\n",
    "len(centers)\n",
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e3a9f42-6215-4523-92a0-f2d54108f702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8.5, 8.5], [0.5, 0.5]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers\n",
    "# [array([0.5, 0.5]), array([8.5, 8.5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc07f4d2-c338-45c0-90eb-50d8353e98da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|newPrediction|\n",
      "+-------------+\n",
      "|            1|\n",
      "|            1|\n",
      "|            0|\n",
      "|            0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if PYSPARK:\n",
    "    transformed = model.transform(df).select(\"features\", \"newPrediction\")\n",
    "else:\n",
    "    transformed = model.transform(df)\n",
    "\n",
    "rows = transformed.collect()\n",
    "transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "649c9e6d-7545-4f6b-ae83-4b89a8e16b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0].newPrediction == rows[1].newPrediction\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a43c8a3f-e2bf-4911-aa92-679e74184b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[2].newPrediction == rows[3].newPrediction\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa25602c-051d-4528-8945-3ee52a078d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    model.hasSummary\n",
    "    # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "943cbdaf-f02d-4767-9392-cac19d54e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    summary = model.summary\n",
    "    summary.k\n",
    "    # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eecb8320-cb7c-463c-b61a-5b11b6b1443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    summary.clusterSizes\n",
    "    # [2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d93e6b6-9911-46dc-b09e-a12726ef955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    summary.trainingCost\n",
    "    # 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a919fe43-1cab-4f47-8b65-6611fde97263",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/tmp\"\n",
    "kmeans_path = temp_path + \"/kmeans\"\n",
    "shutil.rmtree(kmeans_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "698fe092-7d62-4e16-9b6a-6325956437de",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.save(kmeans_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "351c1ebe-fffa-4d57-9982-06d2c3baacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans2 = KMeans.load(kmeans_path)\n",
    "\n",
    "if PYSPARK:\n",
    "    kmeans2.getK()\n",
    "    # 2\n",
    "else:\n",
    "    # AttributeError: 'KMeans' object has no attribute 'getK'\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3849ee2-3785-468f-a92b-f770d0ac5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = temp_path + \"/kmeans_model\"\n",
    "shutil.rmtree(model_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1de7c77a-42a1-40fc-a2e8-afd287ed99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13e4b816-1cbb-406d-860b-73bdc054b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = KMeansModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6abdfca-a24d-43e9-a2a7-88499429ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    model2.hasSummary\n",
    "    # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "367a1164-f60e-4507-9e53-a7a99cc812aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if PYSPARK:\n",
    "    model.clusterCenters()[0] == model2.clusterCenters()[0]\n",
    "    # array([ True,  True], dtype=bool)\n",
    "else:\n",
    "    print(model.cluster_centers_[0] == model2.cluster_centers_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "54baf487-9c5f-4959-8c36-02a1d492336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if PYSPARK:\n",
    "    model.clusterCenters()[1] == model2.clusterCenters()[1]\n",
    "    # array([ True,  True], dtype=bool)\n",
    "else:\n",
    "    print(model.cluster_centers_[1] == model2.cluster_centers_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4a6c8a6d-92eb-4288-9fe0-79ab9e903332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(df).take(1) == model2.transform(df).take(1)\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa86508c-685e-4dcb-8280-68b20c6d485d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(newPrediction=1)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(df).take(1)\n",
    "# [Row(features=DenseVector([0.0, 0.0]), weighCol=2.0, newPrediction=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea1d778-7431-4137-a704-ae42ad6f1ea2",
   "metadata": {},
   "source": [
    "## LinearRegression\n",
    "\n",
    "From: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html#pyspark.ml.regression.LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "37e14233-ed46-4c33-919c-5b5afd16aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYSPARK = False\n",
    "SPARK_RAPIDS_ML = not PYSPARK\n",
    "\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a2b9e647-50c3-4bd0-874b-8989e467185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "else:\n",
    "    from sparkcuml.regression import LinearRegression, LinearRegressionModel\n",
    "\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1c6e94f2-9e9b-45b5-82b6-3bd61a410b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+\n",
      "|label|weight|features|\n",
      "+-----+------+--------+\n",
      "|  1.0|   2.0|   [1.0]|\n",
      "|  0.0|   2.0|   [0.0]|\n",
      "+-----+------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('label', DoubleType(), True), StructField('weight', DoubleType(), True), StructField('features', ArrayType(DoubleType(), True), True)])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if PYSPARK:\n",
    "    df = spark.createDataFrame([\n",
    "        (1.0, 2.0, Vectors.dense(1.0)),\n",
    "        (0.0, 2.0, Vectors.sparse(1, [], []))], [\"label\", \"weight\", \"features\"])\n",
    "else:\n",
    "    # Vectors not supported in this revision of code\n",
    "    data = [(1.0, 2.0, [1.0]),\n",
    "            (0.0, 2.0, [0.0])]\n",
    "    pdf = pd.DataFrame(data)\n",
    "    df = spark.createDataFrame(pdf, [\"label\", \"weight\", \"features\"]).repartition(1)\n",
    "\n",
    "df.show(); df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bb5ff051-ca94-4f09-a546-74fd48aaaae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    lr = LinearRegression(regParam=0.0, solver=\"normal\", weightCol=\"weight\")\n",
    "else:\n",
    "    # need to use cuML 'solver' options here\n",
    "    lr = LinearRegression(regParam=0.0, solver=\"eig\", max_iter=5, inputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "49cf4177-6cc7-4fbc-91cf-151dfcab23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    lr.setMaxIter(5)\n",
    "    lr.getMaxIter()\n",
    "    # 5\n",
    "else:\n",
    "    # Note: no cuML parameter setters/getters\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8895e735-b09f-49b2-a342-29f2405e94f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param algorithm (default: eig)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "fit_intercept: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param fit_intercept (default: True)\n",
      "inputCol: input column name. (current: features)\n",
      "inputCols: input column names. (undefined)\n",
      "labelCol: label column name. (default: label)\n",
      "loss: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param loss (default: squared_loss)\n",
      "max_iter: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param max_iter (default: 1000, current: 5)\n",
      "normalize: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param normalize (default: False)\n",
      "num_workers: The number of Spark CUML workers. Each CUML worker corresponds to one spark task. (default: 1)\n",
      "outputCol: output column name. (default: LinearRegression_d12dbeb95e6a__output)\n",
      "outputCols: output column names. (undefined)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.0)\n",
      "shuffle: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param shuffle (default: True)\n",
      "solver: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param solver (default: eig, current: eig)\n",
      "tol: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param tol (default: 0.001)\n",
      "verbose: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param verbose (default: False)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50d3e6b0-14f6-4e9f-85c5-3fab4335f49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.setRegParam(0.1)\n",
    "lr.getRegParam()\n",
    "# 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e35968e1-8af4-4664-994c-83fab9dae385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_d12dbeb95e6a"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.setRegParam(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9f8280b3-a086-472d-8e87-7d52d9cd72d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param algorithm (default: eig)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "fit_intercept: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param fit_intercept (default: True)\n",
      "inputCol: input column name. (current: features)\n",
      "inputCols: input column names. (undefined)\n",
      "labelCol: label column name. (default: label)\n",
      "loss: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param loss (default: squared_loss)\n",
      "max_iter: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param max_iter (default: 1000, current: 5)\n",
      "normalize: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param normalize (default: False)\n",
      "num_workers: The number of Spark CUML workers. Each CUML worker corresponds to one spark task. (default: 1)\n",
      "outputCol: output column name. (default: LinearRegression_d12dbeb95e6a__output)\n",
      "outputCols: output column names. (undefined)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.0)\n",
      "shuffle: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param shuffle (default: True)\n",
      "solver: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param solver (default: eig, current: eig)\n",
      "tol: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param tol (default: 0.001)\n",
      "verbose: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param verbose (default: False)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3abe2f15-14b5-43a1-9521-400d3cf56de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/01 13:18:32 WARN TaskSetManager: Lost task 0.0 in stage 59.0 (TID 168) (192.168.86.223 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/dev/nvspark/spark-cuml/src/sparkcuml/core.py\", line 447, in _train_udf\n",
      "    logger.info(\"Cuml fit complete\")\n",
      "  File \"/home/leey/dev/nvspark/spark-cuml/src/sparkcuml/regression.py\", line 167, in _linear_regression_fit\n",
      "    linear_regression.fit(\n",
      "  File \"/home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/internals/api_decorators.py\", line 415, in inner\n",
      "    return func(*args, **kwargs)\n",
      "  File \"base_mg.pyx\", line 90, in cuml.linear_model.base_mg.MGFitMixin.fit\n",
      "  File \"/home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/internals/api_decorators.py\", line 415, in inner\n",
      "    return func(*args, **kwargs)\n",
      "  File \"linear_regression_mg.pyx\", line 94, in cuml.linear_model.linear_regression_mg.LinearRegressionMG._fit\n",
      "RuntimeError: exception occured! file=/workspace/.conda-bld/work/cpp/src/glm/ols_mg.cu line=78: olsFit: no algorithm with this id has been implemented\n",
      "Obtained 61 stack frames\n",
      "#0 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN4raft9exception18collect_call_stackEv+0x3b) [0x7f1589b9218b]\n",
      "#1 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN4raft9exceptionC2ENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE+0x61) [0x7f1589b928e1]\n",
      "#2 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN2ML3OLS3opg8fit_implIdEEvRN4raft8handle_tERSt6vectorIPN8MLCommon6Matrix4DataIT_EESaISC_EERNS8_14PartDescriptorESF_PSA_SI_bbiPP11CUstream_stib+0x324) [0x7f158a5bf314]\n",
      "#3 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN2ML3OLS3opg8fit_implIdEEvRN4raft8handle_tERSt6vectorIPN8MLCommon6Matrix4DataIT_EESaISC_EERNS8_14PartDescriptorESF_PSA_SI_bbib+0x138) [0x7f158a5bfcc8]\n",
      "#4 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/linear_regression_mg.cpython-39-x86_64-linux-gnu.so(+0x1e1b6) [0x7f15403101b6]\n",
      "#5 in python3(PyObject_Call+0x157) [0x5627e9e63997]\n",
      "#6 in python3(_PyEval_EvalFrameDefault+0x407d) [0x5627e9e4690d]\n",
      "#7 in python3(+0x12a8b7) [0x5627e9e418b7]\n",
      "#8 in python3(+0x14c198) [0x5627e9e63198]\n",
      "#9 in python3(PyVectorcall_Call+0x87) [0x5627e9e63b77]\n",
      "#10 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/base_mg.cpython-39-x86_64-linux-gnu.so(+0x1728f) [0x7f15402b828f]\n",
      "#11 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/base_mg.cpython-39-x86_64-linux-gnu.so(+0x1daca) [0x7f15402beaca]\n",
      "#12 in python3(PyObject_Call+0x157) [0x5627e9e63997]\n",
      "#13 in python3(_PyEval_EvalFrameDefault+0x407d) [0x5627e9e4690d]\n",
      "#14 in python3(+0x12a8b7) [0x5627e9e418b7]\n",
      "#15 in python3(+0x14c0ff) [0x5627e9e630ff]\n",
      "#16 in python3(_PyEval_EvalFrameDefault+0x4c51) [0x5627e9e474e1]\n",
      "#17 in python3(+0x12a8b7) [0x5627e9e418b7]\n",
      "#18 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n",
      "#19 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n",
      "#20 in python3(+0x1538f4) [0x5627e9e6a8f4]\n",
      "#21 in python3(+0x18ca23) [0x5627e9ea3a23]\n",
      "#22 in python3(+0x18ca23) [0x5627e9ea3a23]\n",
      "#23 in python3(_PyEval_EvalFrameDefault+0x932) [0x5627e9e431c2]\n",
      "#24 in python3(+0x1538f4) [0x5627e9e6a8f4]\n",
      "#25 in python3(_PyEval_EvalFrameDefault+0x932) [0x5627e9e431c2]\n",
      "#26 in python3(+0x1538f4) [0x5627e9e6a8f4]\n",
      "#27 in python3(_PyEval_EvalFrameDefault+0x932) [0x5627e9e431c2]\n",
      "#28 in python3(+0x13d113) [0x5627e9e54113]\n",
      "#29 in python3(_PyEval_EvalFrameDefault+0x4c51) [0x5627e9e474e1]\n",
      "#30 in python3(+0x12a8b7) [0x5627e9e418b7]\n",
      "#31 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n",
      "#32 in python3(_PyEval_EvalFrameDefault+0x66e) [0x5627e9e42efe]\n",
      "#33 in python3(+0x12a8b7) [0x5627e9e418b7]\n",
      "#34 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n",
      "#35 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n",
      "#36 in python3(+0x12a8b7) [0x5627e9e418b7]\n",
      "#37 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n",
      "#38 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n",
      "#39 in python3(+0x13d113) [0x5627e9e54113]\n",
      "#40 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n",
      "#41 in python3(+0x12a8b7) [0x5627e9e418b7]\n",
      "#42 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n",
      "#43 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n",
      "#44 in python3(+0x12a8b7) [0x5627e9e418b7]\n",
      "#45 in python3(_PyEval_EvalCodeWithName+0x47) [0x5627e9e41577]\n",
      "#46 in python3(PyEval_EvalCodeEx+0x39) [0x5627e9e41529]\n",
      "#47 in python3(PyEval_EvalCode+0x1b) [0x5627e9efccdb]\n",
      "#48 in python3(+0x1ea76d) [0x5627e9f0176d]\n",
      "#49 in python3(+0x13d79d) [0x5627e9e5479d]\n",
      "#50 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n",
      "#51 in python3(+0x12a8b7) [0x5627e9e418b7]\n",
      "#52 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n",
      "#53 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n",
      "#54 in python3(+0x12a8b7) [0x5627e9e418b7]\n",
      "#55 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n",
      "#56 in python3(+0x207a5b) [0x5627e9f1ea5b]\n",
      "#57 in python3(Py_RunMain+0xcc) [0x5627e9f1df9c]\n",
      "#58 in python3(Py_BytesMain+0x39) [0x5627e9ef0979]\n",
      "#59 in /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x7f16567900b3]\n",
      "#60 in python3(+0x1d9881) [0x5627e9ef0881]\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:554)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:507)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:727)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2079)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:267)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(59, 0) finished unsuccessfully.\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/leey/dev/nvspark/spark-cuml/src/sparkcuml/core.py\", line 447, in _train_udf\n    logger.info(\"Cuml fit complete\")\n  File \"/home/leey/dev/nvspark/spark-cuml/src/sparkcuml/regression.py\", line 167, in _linear_regression_fit\n    linear_regression.fit(\n  File \"/home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/internals/api_decorators.py\", line 415, in inner\n    return func(*args, **kwargs)\n  File \"base_mg.pyx\", line 90, in cuml.linear_model.base_mg.MGFitMixin.fit\n  File \"/home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/internals/api_decorators.py\", line 415, in inner\n    return func(*args, **kwargs)\n  File \"linear_regression_mg.pyx\", line 94, in cuml.linear_model.linear_regression_mg.LinearRegressionMG._fit\nRuntimeError: exception occured! file=/workspace/.conda-bld/work/cpp/src/glm/ols_mg.cu line=78: olsFit: no algorithm with this id has been implemented\nObtained 61 stack frames\n#0 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN4raft9exception18collect_call_stackEv+0x3b) [0x7f1589b9218b]\n#1 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN4raft9exceptionC2ENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE+0x61) [0x7f1589b928e1]\n#2 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN2ML3OLS3opg8fit_implIdEEvRN4raft8handle_tERSt6vectorIPN8MLCommon6Matrix4DataIT_EESaISC_EERNS8_14PartDescriptorESF_PSA_SI_bbiPP11CUstream_stib+0x324) [0x7f158a5bf314]\n#3 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN2ML3OLS3opg8fit_implIdEEvRN4raft8handle_tERSt6vectorIPN8MLCommon6Matrix4DataIT_EESaISC_EERNS8_14PartDescriptorESF_PSA_SI_bbib+0x138) [0x7f158a5bfcc8]\n#4 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/linear_regression_mg.cpython-39-x86_64-linux-gnu.so(+0x1e1b6) [0x7f15403101b6]\n#5 in python3(PyObject_Call+0x157) [0x5627e9e63997]\n#6 in python3(_PyEval_EvalFrameDefault+0x407d) [0x5627e9e4690d]\n#7 in python3(+0x12a8b7) [0x5627e9e418b7]\n#8 in python3(+0x14c198) [0x5627e9e63198]\n#9 in python3(PyVectorcall_Call+0x87) [0x5627e9e63b77]\n#10 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/base_mg.cpython-39-x86_64-linux-gnu.so(+0x1728f) [0x7f15402b828f]\n#11 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/base_mg.cpython-39-x86_64-linux-gnu.so(+0x1daca) [0x7f15402beaca]\n#12 in python3(PyObject_Call+0x157) [0x5627e9e63997]\n#13 in python3(_PyEval_EvalFrameDefault+0x407d) [0x5627e9e4690d]\n#14 in python3(+0x12a8b7) [0x5627e9e418b7]\n#15 in python3(+0x14c0ff) [0x5627e9e630ff]\n#16 in python3(_PyEval_EvalFrameDefault+0x4c51) [0x5627e9e474e1]\n#17 in python3(+0x12a8b7) [0x5627e9e418b7]\n#18 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n#19 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n#20 in python3(+0x1538f4) [0x5627e9e6a8f4]\n#21 in python3(+0x18ca23) [0x5627e9ea3a23]\n#22 in python3(+0x18ca23) [0x5627e9ea3a23]\n#23 in python3(_PyEval_EvalFrameDefault+0x932) [0x5627e9e431c2]\n#24 in python3(+0x1538f4) [0x5627e9e6a8f4]\n#25 in python3(_PyEval_EvalFrameDefault+0x932) [0x5627e9e431c2]\n#26 in python3(+0x1538f4) [0x5627e9e6a8f4]\n#27 in python3(_PyEval_EvalFrameDefault+0x932) [0x5627e9e431c2]\n#28 in python3(+0x13d113) [0x5627e9e54113]\n#29 in python3(_PyEval_EvalFrameDefault+0x4c51) [0x5627e9e474e1]\n#30 in python3(+0x12a8b7) [0x5627e9e418b7]\n#31 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n#32 in python3(_PyEval_EvalFrameDefault+0x66e) [0x5627e9e42efe]\n#33 in python3(+0x12a8b7) [0x5627e9e418b7]\n#34 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n#35 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n#36 in python3(+0x12a8b7) [0x5627e9e418b7]\n#37 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n#38 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n#39 in python3(+0x13d113) [0x5627e9e54113]\n#40 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n#41 in python3(+0x12a8b7) [0x5627e9e418b7]\n#42 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n#43 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n#44 in python3(+0x12a8b7) [0x5627e9e418b7]\n#45 in python3(_PyEval_EvalCodeWithName+0x47) [0x5627e9e41577]\n#46 in python3(PyEval_EvalCodeEx+0x39) [0x5627e9e41529]\n#47 in python3(PyEval_EvalCode+0x1b) [0x5627e9efccdb]\n#48 in python3(+0x1ea76d) [0x5627e9f0176d]\n#49 in python3(+0x13d79d) [0x5627e9e5479d]\n#50 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n#51 in python3(+0x12a8b7) [0x5627e9e418b7]\n#52 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n#53 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n#54 in python3(+0x12a8b7) [0x5627e9e418b7]\n#55 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n#56 in python3(+0x207a5b) [0x5627e9f1ea5b]\n#57 in python3(Py_RunMain+0xcc) [0x5627e9f1df9c]\n#58 in python3(Py_BytesMain+0x39) [0x5627e9ef0979]\n#59 in /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x7f16567900b3]\n#60 in python3(+0x1d9881) [0x5627e9ef0881]\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:554)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:507)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:727)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:433)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2079)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:267)\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2789)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2725)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2724)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2724)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2162)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2916)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2262)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2283)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2302)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2327)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/nvspark/spark-cuml/src/sparkcuml/core.py:454\u001b[0m, in \u001b[0;36m_CumlEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mpartitionId() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mresult)\n\u001b[1;32m    453\u001b[0m ret \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 454\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapInPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_train_udf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_out_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbarrier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    458\u001b[0m )\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_pyspark_model(ret))\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/pyspark/rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1814\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/pyspark/errors/exceptions.py:219\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    221\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(59, 0) finished unsuccessfully.\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/leey/dev/nvspark/spark-cuml/src/sparkcuml/core.py\", line 447, in _train_udf\n    logger.info(\"Cuml fit complete\")\n  File \"/home/leey/dev/nvspark/spark-cuml/src/sparkcuml/regression.py\", line 167, in _linear_regression_fit\n    linear_regression.fit(\n  File \"/home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/internals/api_decorators.py\", line 415, in inner\n    return func(*args, **kwargs)\n  File \"base_mg.pyx\", line 90, in cuml.linear_model.base_mg.MGFitMixin.fit\n  File \"/home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/internals/api_decorators.py\", line 415, in inner\n    return func(*args, **kwargs)\n  File \"linear_regression_mg.pyx\", line 94, in cuml.linear_model.linear_regression_mg.LinearRegressionMG._fit\nRuntimeError: exception occured! file=/workspace/.conda-bld/work/cpp/src/glm/ols_mg.cu line=78: olsFit: no algorithm with this id has been implemented\nObtained 61 stack frames\n#0 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN4raft9exception18collect_call_stackEv+0x3b) [0x7f1589b9218b]\n#1 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN4raft9exceptionC2ENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE+0x61) [0x7f1589b928e1]\n#2 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN2ML3OLS3opg8fit_implIdEEvRN4raft8handle_tERSt6vectorIPN8MLCommon6Matrix4DataIT_EESaISC_EERNS8_14PartDescriptorESF_PSA_SI_bbiPP11CUstream_stib+0x324) [0x7f158a5bf314]\n#3 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN2ML3OLS3opg8fit_implIdEEvRN4raft8handle_tERSt6vectorIPN8MLCommon6Matrix4DataIT_EESaISC_EERNS8_14PartDescriptorESF_PSA_SI_bbib+0x138) [0x7f158a5bfcc8]\n#4 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/linear_regression_mg.cpython-39-x86_64-linux-gnu.so(+0x1e1b6) [0x7f15403101b6]\n#5 in python3(PyObject_Call+0x157) [0x5627e9e63997]\n#6 in python3(_PyEval_EvalFrameDefault+0x407d) [0x5627e9e4690d]\n#7 in python3(+0x12a8b7) [0x5627e9e418b7]\n#8 in python3(+0x14c198) [0x5627e9e63198]\n#9 in python3(PyVectorcall_Call+0x87) [0x5627e9e63b77]\n#10 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/base_mg.cpython-39-x86_64-linux-gnu.so(+0x1728f) [0x7f15402b828f]\n#11 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/base_mg.cpython-39-x86_64-linux-gnu.so(+0x1daca) [0x7f15402beaca]\n#12 in python3(PyObject_Call+0x157) [0x5627e9e63997]\n#13 in python3(_PyEval_EvalFrameDefault+0x407d) [0x5627e9e4690d]\n#14 in python3(+0x12a8b7) [0x5627e9e418b7]\n#15 in python3(+0x14c0ff) [0x5627e9e630ff]\n#16 in python3(_PyEval_EvalFrameDefault+0x4c51) [0x5627e9e474e1]\n#17 in python3(+0x12a8b7) [0x5627e9e418b7]\n#18 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n#19 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n#20 in python3(+0x1538f4) [0x5627e9e6a8f4]\n#21 in python3(+0x18ca23) [0x5627e9ea3a23]\n#22 in python3(+0x18ca23) [0x5627e9ea3a23]\n#23 in python3(_PyEval_EvalFrameDefault+0x932) [0x5627e9e431c2]\n#24 in python3(+0x1538f4) [0x5627e9e6a8f4]\n#25 in python3(_PyEval_EvalFrameDefault+0x932) [0x5627e9e431c2]\n#26 in python3(+0x1538f4) [0x5627e9e6a8f4]\n#27 in python3(_PyEval_EvalFrameDefault+0x932) [0x5627e9e431c2]\n#28 in python3(+0x13d113) [0x5627e9e54113]\n#29 in python3(_PyEval_EvalFrameDefault+0x4c51) [0x5627e9e474e1]\n#30 in python3(+0x12a8b7) [0x5627e9e418b7]\n#31 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n#32 in python3(_PyEval_EvalFrameDefault+0x66e) [0x5627e9e42efe]\n#33 in python3(+0x12a8b7) [0x5627e9e418b7]\n#34 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n#35 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n#36 in python3(+0x12a8b7) [0x5627e9e418b7]\n#37 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n#38 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n#39 in python3(+0x13d113) [0x5627e9e54113]\n#40 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n#41 in python3(+0x12a8b7) [0x5627e9e418b7]\n#42 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n#43 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n#44 in python3(+0x12a8b7) [0x5627e9e418b7]\n#45 in python3(_PyEval_EvalCodeWithName+0x47) [0x5627e9e41577]\n#46 in python3(PyEval_EvalCodeEx+0x39) [0x5627e9e41529]\n#47 in python3(PyEval_EvalCode+0x1b) [0x5627e9efccdb]\n#48 in python3(+0x1ea76d) [0x5627e9f0176d]\n#49 in python3(+0x13d79d) [0x5627e9e5479d]\n#50 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n#51 in python3(+0x12a8b7) [0x5627e9e418b7]\n#52 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n#53 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x5627e9e42c4f]\n#54 in python3(+0x12a8b7) [0x5627e9e418b7]\n#55 in python3(_PyFunction_Vectorcall+0xb9) [0x5627e9e53e09]\n#56 in python3(+0x207a5b) [0x5627e9f1ea5b]\n#57 in python3(Py_RunMain+0xcc) [0x5627e9f1df9c]\n#58 in python3(Py_BytesMain+0x39) [0x5627e9ef0979]\n#59 in /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x7f16567900b3]\n#60 in python3(+0x1d9881) [0x5627e9ef0881]\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:554)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:507)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:727)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:433)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2079)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:267)\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2789)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2725)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2724)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2724)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2162)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2916)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2262)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2283)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2302)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2327)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "model = lr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ecfce-0314-435a-b480-f6b5a930226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    model.setFeaturesCol(\"features\")\n",
    "    model.setPredictionCol(\"newPrediction\")\n",
    "    model.getMaxIter()\n",
    "    # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6db09-c0c6-4651-903c-295adc035612",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.getMaxBlockSizeInMB()\n",
    "# 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f699f7-41f2-4558-8e09-f76c4aa49bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    test0 = spark.createDataFrame([(Vectors.dense(1.0),)], [\"features\"])\n",
    "else:\n",
    "    test0 = spark.createDataFrame([([1.0],)], [\"features\"])\n",
    "\n",
    "test0.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b3930-9f59-442b-b0ed-2a429724e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(model.predict(test0.head().features) - (-1.0)) < 0.001\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf88ae4-671f-4ec1-9fa7-31a4d84cd46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(model.transform(test0).head().newPrediction - (-1.0)) < 0.001\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b88b68-904e-471f-8fb3-692c779d5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(model.coefficients[0] - 1.0) < 0.001\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77feec-cbb1-4680-a365-528645c4b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(model.intercept - 0.0) < 0.001\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b792e-9601-4de6-a114-8d329fc9f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n",
    "abs(model.transform(test1).head().newPrediction - 1.0) < 0.001\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b59367-2302-446c-8348-e2b510c2a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.setParams(featuresCol=\"vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e07233-807e-479a-883c-f9097dcd592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/tmp\"\n",
    "lr_path = temp_path + \"/lr\"\n",
    "shutil.rmtree(lr_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9631c094-5321-4d41-acdc-b2752979d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.save(lr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4dc2f-e09b-47f3-a0a3-abd3d9e174b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LinearRegression.load(lr_path)\n",
    "lr2.getMaxIter()\n",
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9e46c-0c88-4c0c-9f8e-ac0ad457ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = temp_path + \"/lr_model\"\n",
    "shutil.rmtree(model_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80def2f-34d2-427e-826f-59c6e7c906e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3853568-f176-42d0-afad-25f69756ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LinearRegressionModel.load(model_path)\n",
    "model.coefficients[0] == model2.coefficients[0]\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf13eb1-7be8-443c-89d3-955be5044e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept == model2.intercept\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f3e4a-fcaf-4390-90d3-55e0d9b0f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform(test0).take(1) == model2.transform(test0).take(1)\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ac2c7-6f7e-4eef-95fc-25cf684afa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.numFeatures\n",
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c82d92-8826-4e1a-bf8b-295cf4efcd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(model_path + \"_2\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613cd8d-4234-46f7-9c85-04e6ccd8c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().format(\"pmml\").save(model_path + \"_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd0d49a-d7b2-4343-aca5-5b301c20a74e",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e65572d4-8bd7-42d7-9bc0-b68c8f800073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.functions import array\n",
    "from sparkcuml.regression import LinearRegression, LinearRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7130d47c-f6b4-436d-ab72-f0d82f29caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(\n",
    "    [[-0.20515826,  1.4940791 ],\n",
    "     [ 0.12167501,  0.7610377 ],\n",
    "     [ 1.4542735,   0.14404356],\n",
    "     [-0.85409576,  0.3130677 ],\n",
    "     [ 2.2408931,   0.978738  ],\n",
    "     [-0.1513572,   0.95008844],\n",
    "     [-0.9772779,   1.867558  ],\n",
    "     [ 0.41059852, -0.10321885]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0335f18f-d139-41ca-b094-047e40a5e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([2.0374513, 22.403986, 139.4456, -76.19584, 225.72075, -0.6784152, -65.54835, 37.30829])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cfd78983-a0a3-48a2-a4e5-bdfceec357d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, n = X.shape\n",
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "30c9d584-66c2-43a9-9cd4-aafad4bb2e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [f\"c{i}\" for i in range(n)]\n",
    "schema = [f\"{c} float\" for c in feature_cols]\n",
    "\n",
    "label_col = \"label_col\"\n",
    "schema.append(\"label_col float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "11f4a34c-cc6c-440d-b289-e59fa2c5a57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['c0', 'c1'], 'label_col')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols, label_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "29e50450-1869-496e-a5ff-b6078c85b454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c0 float', 'c1 float', 'label_col float']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2ec95d8d-30e0-4faf-b5ff-9516a084c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    np.concatenate((X, y.reshape(m, 1)), axis=1).tolist(),\n",
    "    \",\".join(schema),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d049829-6e03-485c-8b24-6aba05ac5834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+\n",
      "|         c0|         c1| label_col|\n",
      "+-----------+-----------+----------+\n",
      "|-0.20515826|  1.4940791| 2.0374513|\n",
      "| 0.12167501|  0.7610377| 22.403986|\n",
      "|  1.4542735| 0.14404356|  139.4456|\n",
      "|-0.85409576|  0.3130677| -76.19584|\n",
      "|  2.2408931|   0.978738| 225.72075|\n",
      "| -0.1513572| 0.95008844|-0.6784152|\n",
      "| -0.9772779|   1.867558| -65.54835|\n",
      "| 0.41059852|-0.10321885|  37.30829|\n",
      "+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7c29dad4-c743-4ac7-b6f2-b793613ee381",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"features\", array(*feature_cols)).drop(*feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d1b2aee9-6366-4e1d-a942-5569cf141b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "| label_col|            features|\n",
      "+----------+--------------------+\n",
      "| 2.0374513|[-0.20515826, 1.4...|\n",
      "| 22.403986|[0.12167501, 0.76...|\n",
      "|  139.4456|[1.4542735, 0.144...|\n",
      "| -76.19584|[-0.85409576, 0.3...|\n",
      "| 225.72075|[2.2408931, 0.978...|\n",
      "|-0.6784152|[-0.1513572, 0.95...|\n",
      "| -65.54835|[-0.9772779, 1.86...|\n",
      "|  37.30829|[0.41059852, -0.1...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "72811822-4182-4403-95b1-6bf643d1b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ebde896d-8b40-4ba4-ae98-1af2e1a9f787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_c904beda5155"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.setRegParam(0.0)\n",
    "lr.setFeaturesCol(\"features\")\n",
    "lr.setLabelCol(\"label_col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d359631e-f50d-4173-939e-f04794512b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param algorithm (default: eig)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "fit_intercept: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param fit_intercept (default: True)\n",
      "inputCol: input column name. (current: features)\n",
      "inputCols: input column names. (undefined)\n",
      "labelCol: label column name. (default: label, current: label_col)\n",
      "loss: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param loss (default: squared_loss)\n",
      "max_iter: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param max_iter (default: 1000)\n",
      "normalize: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param normalize (default: False)\n",
      "num_workers: The number of Spark CUML workers. Each CUML worker corresponds to one spark task. (default: 1)\n",
      "outputCol: output column name. (default: LinearRegression_c904beda5155__output)\n",
      "outputCols: output column names. (undefined)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.0)\n",
      "shuffle: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param shuffle (default: True)\n",
      "solver: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param solver (default: eig)\n",
      "tol: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param tol (default: 0.001)\n",
      "verbose: Refer to CUML doc of cuml.linear_model.linear_regression.LinearRegression, cuml.linear_model.ridge.Ridge, cuml.solvers.cd.CD for this param verbose (default: False)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "694314a4-a707-4b1b-86f5-ce645ae881bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9bd54034-cd9d-486a-ab21-d5819d7264a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[94.46688842773438, 14.3353271484375]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1ff3e0-fd50-428c-aed8-df6c3cd01d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
