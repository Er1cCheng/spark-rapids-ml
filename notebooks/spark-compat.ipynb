{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4319ad5-7b8d-47ae-8227-230ce6ee40ec",
   "metadata": {},
   "source": [
    "# Pyspark Compatibility Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df866f0-8ca1-458a-a427-f01ffcdd77ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PCA\n",
    "From: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.PCA.html#pyspark.ml.feature.PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d73ca9e5-d033-435b-8f8c-2aa64ee2a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYSPARK = False\n",
    "SPARK_RAPIDS_ML = not PYSPARK\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65bb00c7-7aba-45a6-8998-3182051b2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    from pyspark.ml.feature import PCA, PCAModel\n",
    "else:\n",
    "    from spark_rapids_ml.feature import PCA, PCAModel\n",
    "\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f56d981-48b1-4daf-acb6-8ba4fdba06a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(SparseVector(5, {1: 1.0, 3: 7.0}),),\n",
       " (DenseVector([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
       " (DenseVector([4.0, 0.0, 0.0, 6.0, 7.0]),)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n",
    "        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n",
    "        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae01021-9ccd-4545-a054-f61c39c6b224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "| (5,[1,3],[1.0,7.0])|\n",
      "|[2.0,0.0,3.0,4.0,...|\n",
      "|[4.0,0.0,0.0,6.0,...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('features', VectorUDT(), True)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(data,[\"features\"])\n",
    "df.show(); df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63028b55-5642-4552-9c3d-347e26466956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputCol: input column name. (undefined)\n",
      "inputCols: input column names. (undefined)\n",
      "k: the number of principal components (undefined)\n",
      "num_workers: (cuML) number of Spark CuML workers, where each CuML worker corresponds to one Spark task. (default: 1)\n",
      "outputCol: output column name. (default: PCA_10f6851bdaeb__output)\n",
      "outputCols: output column names. (undefined)\n"
     ]
    }
   ],
   "source": [
    "pca = PCA()\n",
    "print(pca.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2830b959-ff86-40f8-8595-d1ec683eb5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA_75286c798362"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(k=2, inputCol=\"features\")\n",
    "# pca = PCA(k=2, inputCol=\"features\", n_components=3)\n",
    "# pca = PCA(inputCol=\"features\", n_components=3)\n",
    "pca.setOutputCol(\"pca_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966803e8-8a96-4f0f-a046-0eecdd0c0769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputCol: input column name. (current: features)\n",
      "inputCols: input column names. (undefined)\n",
      "k: the number of principal components (current: 2)\n",
      "num_workers: (cuML) number of Spark CuML workers, where each CuML worker corresponds to one Spark task. (default: 1)\n",
      "outputCol: output column name. (default: PCA_75286c798362__output, current: pca_features)\n",
      "outputCols: output column names. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(pca.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc4a9cc-fd81-4fe5-9eee-dd8949bed51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'k': 'n_components'}\n",
      "{'n_components': 2, 'svd_solver': 'auto', 'verbose': False, 'whiten': False}\n"
     ]
    }
   ],
   "source": [
    "if SPARK_RAPIDS_ML:\n",
    "    print(pca._param_mapping())\n",
    "    print(pca.cuml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63804960-9350-4852-a955-fa8ee7a11edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA_75286c798362"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.setK(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90926a3d-fe7b-48d2-99b5-960484b165ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputCol: input column name. (current: features)\n",
      "inputCols: input column names. (undefined)\n",
      "k: the number of principal components (current: 3)\n",
      "num_workers: (cuML) number of Spark CuML workers, where each CuML worker corresponds to one Spark task. (default: 1)\n",
      "outputCol: output column name. (default: PCA_75286c798362__output, current: pca_features)\n",
      "outputCols: output column names. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(pca.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6950622c-c249-4a9d-894e-78cfe9734b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_components': 3, 'svd_solver': 'auto', 'verbose': False, 'whiten': False}\n"
     ]
    }
   ],
   "source": [
    "if SPARK_RAPIDS_ML:\n",
    "    print(pca.cuml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1cc9410-d6ea-4933-ae9c-ceb65833ad34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA_75286c798362"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.setK(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a61234f5-070c-4fb4-ad2f-c0b741cf79ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = pca.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3e74f81-476d-4423-bbe5-46b59480d749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.getK()\n",
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a70f79c-2f66-4a5e-82a9-c2cfe02f490e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCAModel_d59102329b2f"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.setOutputCol(\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11ed38a9-743e-4598-9ccf-dc28b2e78ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputCol: input column name. (current: features)\n",
      "inputCols: input column names. (undefined)\n",
      "k: the number of principal components (current: 2)\n",
      "num_workers: (cuML) number of Spark CuML workers, where each CuML worker corresponds to one Spark task. (default: 1)\n",
      "outputCol: output column name. (default: PCA_75286c798362__output, current: output)\n",
      "outputCols: output column names. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(model.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95db536e-ccda-4380-93cd-4d313365a697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_components': 2, 'svd_solver': 'auto', 'verbose': False, 'whiten': False}\n"
     ]
    }
   ],
   "source": [
    "if SPARK_RAPIDS_ML:\n",
    "    print(model.cuml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08e8b235-e7a3-4694-9a62-46e9ec93094d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1.6485728230896184, -4.013282697765595]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(df).collect()[0].output\n",
    "# DenseVector([1.648..., -4.013...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c1db46f-72f8-43f6-aaf8-6aebd4c7318a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([0.7944, 0.2056])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.explainedVariance\n",
    "# DenseVector([0.794..., 0.205...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99f3e554-fb17-44f5-bad2-ae8b306167ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(5, 2, [0.4486, -0.133, 0.1252, -0.2165, 0.8477, -0.2842, -0.0562, 0.7636, -0.5653, -0.1156], False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pc\n",
    "# DenseMatrix(5, 2, [-0.4486, 0.133, -0.1252, 0.2165, -0.8477, -0.2842, -0.0562, 0.7636, -0.5653, -0.1156], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8da1934e-cd40-40d5-8f56-7d711ed06620",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/tmp\"\n",
    "pcaPath = temp_path + \"/pca\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e55bfaa3-a0c4-4f54-9893-79f8f82410bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(pcaPath, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "deea9119-6f4d-4233-8e6a-28e5ccee6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.save(pcaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cd390f1-c709-4f4d-9a83-f86b1e0573e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedPca = PCA.load(pcaPath)\n",
    "loadedPca.getK() == pca.getK()\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cb9a5d4-a09f-435e-933b-59ceadbf92d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_components': 2, 'svd_solver': 'auto', 'verbose': False, 'whiten': False}\n",
      "{'n_components': 2, 'svd_solver': 'auto', 'verbose': False, 'whiten': False}\n"
     ]
    }
   ],
   "source": [
    "# confirm saved estimator cuml_params\n",
    "if SPARK_RAPIDS_ML:\n",
    "    print(pca.cuml_params)\n",
    "    print(loadedPca.cuml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02a6c8a1-d5d8-40af-9804-d600e503bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = temp_path + \"/pca-model\"\n",
    "shutil.rmtree(modelPath, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "353986ab-fba7-48a7-9f97-2db5ce1490ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47aad8bd-b4bb-410d-873d-bdc9ba9d0c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedModel = PCAModel.load(modelPath)\n",
    "loadedModel.pc == model.pc\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e57e11e-4ac5-4145-9351-6577e2e14e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_components': 2, 'svd_solver': 'auto', 'verbose': False, 'whiten': False}\n",
      "{'n_components': 2, 'svd_solver': 'auto', 'verbose': False, 'whiten': False}\n"
     ]
    }
   ],
   "source": [
    "# confirm saved model cuml_params\n",
    "if SPARK_RAPIDS_ML:\n",
    "    print(model.cuml_params)\n",
    "    print(loadedModel.cuml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44681be2-a2a5-4111-8fb0-46a77dfa61dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedModel.explainedVariance == model.explainedVariance\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "081bb7ef-86b6-45e3-88d8-cea892777bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedModel.transform(df).take(1) == model.transform(df).take(1)\n",
    "# True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f3f77-2016-43ae-8b8c-c8fd2b7117e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## KMeans\n",
    "From: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.clustering.KMeans.html#pyspark.ml.clustering.KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "304140a6-06d8-4d53-b3c1-576756f2a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYSPARK = False\n",
    "SPARK_RAPIDS_ML = not PYSPARK\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cc1d3be-9835-4157-b836-a49f4a6d0577",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "else:\n",
    "    from spark_rapids_ml.clustering import KMeans, KMeansModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5f09d6f-66cd-4b1e-9926-b9fe0eb08119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1b3d2b2-91da-4ec5-abbe-19667ac0f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(Vectors.dense([0.0, 0.0]), 2.0), (Vectors.dense([1.0, 1.0]), 2.0),\n",
    "        (Vectors.dense([9.0, 8.0]), 2.0), (Vectors.dense([8.0, 9.0]), 2.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d8017c4-9ca0-40ec-be4c-15aef93368ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "| features|weighCol|\n",
      "+---------+--------+\n",
      "|[0.0,0.0]|     2.0|\n",
      "|[1.0,1.0]|     2.0|\n",
      "|[9.0,8.0]|     2.0|\n",
      "|[8.0,9.0]|     2.0|\n",
      "+---------+--------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('features', VectorUDT(), True), StructField('weighCol', DoubleType(), True)])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(data, [\"features\", \"weighCol\"]).repartition(1)\n",
    "df.show(); df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b77ee895-d2b7-4a6c-a6f1-d66eb10f7b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Spark Param 'distanceMeasure' is not used by CuML.\n",
      "WARNING: Spark Param 'initSteps' is not used by CuML.\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d6c28d8-7ecb-47db-a016-6d8211abb638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distanceMeasure: the distance measure. Supported options: 'euclidean' and 'cosine'. (default: euclidean)\n",
      "featuresCol: features column name. (default: features)\n",
      "featuresCols: features column names for multi-column input. (undefined)\n",
      "initMode: The initialization algorithm. This can be either \"random\" to choose random points as initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++ (default: k-means||)\n",
      "initSteps: The number of steps for k-means|| initialization mode. Must be > 0. (default: 2)\n",
      "k: The number of clusters to create. Must be > 1. (default: 2)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 20)\n",
      "num_workers: (cuML) number of Spark CuML workers, where each CuML worker corresponds to one Spark task. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 1909113551)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, row, block. (default: auto)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 0.0001)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(kmeans.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "edaaa7b6-1e44-42e8-bc34-e2c810a41689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'distanceMeasure': '', 'k': 'n_clusters', 'initSteps': '', 'maxIter': 'max_iter', 'seed': 'random_state', 'tol': 'tol', 'weightCol': None}\n",
      "{'n_clusters': 2, 'max_iter': 20, 'tol': 0.0001, 'verbose': False, 'random_state': 1909113551, 'init': 'scalable-k-means++', 'n_init': 1, 'oversampling_factor': 2.0, 'max_samples_per_batch': 32768}\n"
     ]
    }
   ],
   "source": [
    "if SPARK_RAPIDS_ML:\n",
    "    print(kmeans._param_mapping())\n",
    "    print(kmeans.cuml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "238bde84-42a4-4280-9f7e-c0fc678128e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Spark Param 'distanceMeasure' is not used by CuML.\n",
      "WARNING: Spark Param 'initSteps' is not used by CuML.\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(k=2)\n",
    "kmeans.setSeed(1)\n",
    "kmeans.setMaxIter(10)\n",
    "\n",
    "if PYSPARK:\n",
    "    kmeans.setWeightCol(\"weighCol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c658398a-0285-4438-9abe-0ccd196f625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distanceMeasure: the distance measure. Supported options: 'euclidean' and 'cosine'. (default: euclidean)\n",
      "featuresCol: features column name. (default: features)\n",
      "featuresCols: features column names for multi-column input. (undefined)\n",
      "initMode: The initialization algorithm. This can be either \"random\" to choose random points as initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++ (default: k-means||)\n",
      "initSteps: The number of steps for k-means|| initialization mode. Must be > 0. (default: 2)\n",
      "k: The number of clusters to create. Must be > 1. (default: 2, current: 2)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 20, current: 10)\n",
      "num_workers: (cuML) number of Spark CuML workers, where each CuML worker corresponds to one Spark task. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 1909113551, current: 1)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, row, block. (default: auto)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 0.0001)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(kmeans.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "26521ef4-8139-47c0-ad24-0b9c5f786139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_clusters': 2, 'max_iter': 10, 'tol': 0.0001, 'verbose': False, 'random_state': 1, 'init': 'scalable-k-means++', 'n_init': 1, 'oversampling_factor': 2.0, 'max_samples_per_batch': 32768}\n"
     ]
    }
   ],
   "source": [
    "if SPARK_RAPIDS_ML:\n",
    "    print(kmeans.cuml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94b3278f-d5ee-4680-89be-b849b9991a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.getMaxIter()\n",
    "# 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f137bb82-8bd3-4909-9df5-8be5a2d2fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.clear(kmeans.maxIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c155513a-4386-4f08-b8f6-0b9ef2c46947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distanceMeasure: the distance measure. Supported options: 'euclidean' and 'cosine'. (default: euclidean)\n",
      "featuresCol: features column name. (default: features)\n",
      "featuresCols: features column names for multi-column input. (undefined)\n",
      "initMode: The initialization algorithm. This can be either \"random\" to choose random points as initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++ (default: k-means||)\n",
      "initSteps: The number of steps for k-means|| initialization mode. Must be > 0. (default: 2)\n",
      "k: The number of clusters to create. Must be > 1. (default: 2, current: 2)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 20)\n",
      "num_workers: (cuML) number of Spark CuML workers, where each CuML worker corresponds to one Spark task. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "seed: random seed. (default: 1909113551, current: 1)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, row, block. (default: auto)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 0.0001)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(kmeans.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc9bf801-2dc9-47b7-bcad-d43991e8aeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_clusters': 2, 'max_iter': 20, 'tol': 0.0001, 'verbose': False, 'random_state': 1, 'init': 'scalable-k-means++', 'n_init': 1, 'oversampling_factor': 2.0, 'max_samples_per_batch': 32768}\n"
     ]
    }
   ],
   "source": [
    "if SPARK_RAPIDS_ML:\n",
    "    print(kmeans.cuml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b7c2bb5-c827-4df5-bc1a-6275775cf1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans_01693d3ca150"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0aa3a31-c619-4201-81c7-09bbf566e2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Spark Param 'distanceMeasure' is not used by CuML.\n",
      "WARNING: Spark Param 'initSteps' is not used by CuML.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = kmeans.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d8e06e0c-302f-441b-9d74-de0b1b780094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'euclidean'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.getDistanceMeasure()\n",
    "# 'euclidean'\n",
    "# Note: this is not used in spark_rapids_ml (may be implied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3cc0c6a-a09a-4da7-b037-b4101244b780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeansModel_415f60cb48d0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.setPredictionCol(\"newPrediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3915c572-5250-4005-857b-d3beacf03d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distanceMeasure: the distance measure. Supported options: 'euclidean' and 'cosine'. (default: euclidean)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "featuresCols: features column names for multi-column input. (undefined)\n",
      "initMode: The initialization algorithm. This can be either \"random\" to choose random points as initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++ (default: k-means||)\n",
      "initSteps: The number of steps for k-means|| initialization mode. Must be > 0. (default: 2)\n",
      "k: The number of clusters to create. Must be > 1. (default: 2, current: 2)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 20)\n",
      "num_workers: (cuML) number of Spark CuML workers, where each CuML worker corresponds to one Spark task. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction, current: newPrediction)\n",
      "seed: random seed. (default: 1909113551, current: 1)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, row, block. (default: auto)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 0.0001)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(model.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f688351-cf4f-449b-b0ef-7735779e4d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_clusters': 2, 'max_iter': 20, 'tol': 0.0001, 'verbose': False, 'random_state': 1, 'init': 'scalable-k-means++', 'n_init': 1, 'oversampling_factor': 2.0, 'max_samples_per_batch': 32768}\n"
     ]
    }
   ],
   "source": [
    "if SPARK_RAPIDS_ML:\n",
    "    print(model.cuml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a23fe30-d6c3-4e60-b444-9943ca0efdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    model.predict(df.head().features)\n",
    "    # 0\n",
    "else:\n",
    "    # NotImplementedError: 'predict' method is not supported, use 'transform' instead.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "51d7bf2a-7fea-4e06-bea1-b9c28d3fa348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers = model.clusterCenters()\n",
    "len(centers)\n",
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e3a9f42-6215-4523-92a0-f2d54108f702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8.5, 8.5], [0.5, 0.5]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centers\n",
    "# [array([0.5, 0.5]), array([8.5, 8.5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cc07f4d2-c338-45c0-90eb-50d8353e98da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    transformed = model.transform(df).select(\"features\", \"newPrediction\")\n",
    "else:\n",
    "    # AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `features` cannot be resolved. Did you mean one of the following? [`prediction`].;\n",
    "    # 'Project ['features, 'newPrediction]\n",
    "    # +- MapInPandas _transform_udf(weighCol#1, features#29)#35, [prediction#36]\n",
    "    #    +- Project [weighCol#1, features#29]\n",
    "    #       +- Project [cuml_values_c3BhcmtjdW1sCg==#26, weighCol#1, UDF(cuml_values_c3BhcmtjdW1sCg==#26) AS features#29]\n",
    "    #          +- Project [features#0 AS cuml_values_c3BhcmtjdW1sCg==#26, weighCol#1]\n",
    "    #             +- Repartition 1, true\n",
    "    #                +- LogicalRDD [features#0, weighCol#1], false    \n",
    "    transformed = model.transform(df)\n",
    "    \n",
    "rows = transformed.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f5030d2c-aeaa-473e-be60-da10444ffe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------------+\n",
      "|weighCol|  features|newPrediction|\n",
      "+--------+----------+-------------+\n",
      "|     2.0|[0.0, 0.0]|            1|\n",
      "|     2.0|[1.0, 1.0]|            1|\n",
      "|     2.0|[9.0, 8.0]|            0|\n",
      "|     2.0|[8.0, 9.0]|            0|\n",
      "+--------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = model.transform(df)\n",
    "transformed.show()\n",
    "# +---------+--------+-------------+\n",
    "# | features|weighCol|newPrediction|\n",
    "# +---------+--------+-------------+\n",
    "# |[0.0,0.0]|     2.0|            0|\n",
    "# |[1.0,1.0]|     2.0|            0|\n",
    "# |[9.0,8.0]|     2.0|            1|\n",
    "# |[8.0,9.0]|     2.0|            1|\n",
    "# +---------+--------+-------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "649c9e6d-7545-4f6b-ae83-4b89a8e16b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0].newPrediction == rows[1].newPrediction\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a43c8a3f-e2bf-4911-aa92-679e74184b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[2].newPrediction == rows[3].newPrediction\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa25602c-051d-4528-8945-3ee52a078d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hasSummary\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "943cbdaf-f02d-4767-9392-cac19d54e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    summary = model.summary\n",
    "    summary.k\n",
    "    # 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eecb8320-cb7c-463c-b61a-5b11b6b1443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    summary.clusterSizes\n",
    "    # [2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d93e6b6-9911-46dc-b09e-a12726ef955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    summary.trainingCost\n",
    "    # 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a919fe43-1cab-4f47-8b65-6611fde97263",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/tmp\"\n",
    "kmeans_path = temp_path + \"/kmeans\"\n",
    "shutil.rmtree(kmeans_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "698fe092-7d62-4e16-9b6a-6325956437de",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.save(kmeans_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "351c1ebe-fffa-4d57-9982-06d2c3baacdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Spark Param 'distanceMeasure' is not used by CuML.\n",
      "WARNING: Spark Param 'initSteps' is not used by CuML.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans2 = KMeans.load(kmeans_path)\n",
    "kmeans2.getK()\n",
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a5f8f09-f9ae-4a48-b1b8-4b4b21f49af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_clusters': 2, 'max_iter': 20, 'tol': 0.0001, 'verbose': False, 'random_state': 1, 'init': 'scalable-k-means++', 'n_init': 1, 'oversampling_factor': 2.0, 'max_samples_per_batch': 32768}\n",
      "{'n_clusters': 2, 'max_iter': 20, 'tol': 0.0001, 'verbose': False, 'random_state': 1, 'init': 'scalable-k-means++', 'n_init': 1, 'oversampling_factor': 2.0, 'max_samples_per_batch': 32768}\n"
     ]
    }
   ],
   "source": [
    "# confirm saved estimator cuml_params\n",
    "if SPARK_RAPIDS_ML:\n",
    "    print(kmeans.cuml_params)\n",
    "    print(kmeans2.cuml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c3849ee2-3785-468f-a92b-f770d0ac5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = temp_path + \"/kmeans_model\"\n",
    "shutil.rmtree(model_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1de7c77a-42a1-40fc-a2e8-afd287ed99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "13e4b816-1cbb-406d-860b-73bdc054b908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Spark Param 'distanceMeasure' is not used by CuML.\n",
      "WARNING: Spark Param 'initSteps' is not used by CuML.\n"
     ]
    }
   ],
   "source": [
    "model2 = KMeansModel.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "45141fb8-0ded-4813-9ae0-b064954b6690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_clusters': 2, 'max_iter': 20, 'tol': 0.0001, 'verbose': False, 'random_state': 1, 'init': 'scalable-k-means++', 'n_init': 1, 'oversampling_factor': 2.0, 'max_samples_per_batch': 32768}\n",
      "{'n_clusters': 2, 'max_iter': 20, 'tol': 0.0001, 'verbose': False, 'random_state': 1, 'init': 'scalable-k-means++', 'n_init': 1, 'oversampling_factor': 2.0, 'max_samples_per_batch': 32768}\n"
     ]
    }
   ],
   "source": [
    "# confirm saved model cuml_params\n",
    "if SPARK_RAPIDS_ML:\n",
    "    print(model.cuml_params)\n",
    "    print(model2.cuml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b6abdfca-a24d-43e9-a2a7-88499429ce25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.hasSummary\n",
    "# False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "367a1164-f60e-4507-9e53-a7a99cc812aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clusterCenters()[0] == model2.clusterCenters()[0]\n",
    "# array([ True,  True], dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "54baf487-9c5f-4959-8c36-02a1d492336e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clusterCenters()[1] == model2.clusterCenters()[1]\n",
    "# array([ True,  True], dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4a6c8a6d-92eb-4288-9fe0-79ab9e903332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(df).take(1) == model2.transform(df).take(1)\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fa86508c-685e-4dcb-8280-68b20c6d485d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(weighCol=2.0, features=[0.0, 0.0], newPrediction=1)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(df).take(1)\n",
    "# [Row(features=DenseVector([0.0, 0.0]), weighCol=2.0, newPrediction=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4fd543e5-5892-4013-b97a-99d6580e0877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([0.0, 0.0]), weighCol=2.0)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)\n",
    "# [Row(features=DenseVector([0.0, 0.0]), weighCol=2.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea1d778-7431-4137-a704-ae42ad6f1ea2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LinearRegression\n",
    "\n",
    "From: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html#pyspark.ml.regression.LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "37e14233-ed46-4c33-919c-5b5afd16aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYSPARK = False\n",
    "SPARK_RAPIDS_ML = not PYSPARK\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a2b9e647-50c3-4bd0-874b-8989e467185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "else:\n",
    "    from spark_rapids_ml.regression import LinearRegression, LinearRegressionModel\n",
    "\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c6e94f2-9e9b-45b5-82b6-3bd61a410b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+\n",
      "|label|weight| features|\n",
      "+-----+------+---------+\n",
      "|  1.0|   2.0|    [1.0]|\n",
      "|  0.0|   2.0|(1,[],[])|\n",
      "+-----+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('label', DoubleType(), True), StructField('weight', DoubleType(), True), StructField('features', VectorUDT(), True)])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1.0, 2.0, Vectors.dense(1.0)),\n",
    "    (0.0, 2.0, Vectors.sparse(1, [], []))], [\"label\", \"weight\", \"features\"])\n",
    "\n",
    "df.show(); df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3b4a74fc-f83c-413f-8302-c53a39153ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Spark Param 'aggregationDepth' is not used by CuML.\n",
      "WARNING: Spark Param 'epsilon' is not used by CuML.\n",
      "WARNING: Spark Param 'maxBlockSizeInMB' is not used by CuML.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if PYSPARK:\n",
    "    lr = LinearRegression(regParam=0.0, solver=\"normal\", weightCol=\"weight\")\n",
    "else:\n",
    "    # 'solver: normal' gets value mapped to 'solver: eig'\n",
    "    # 'weightCol` is explicitly not supported\n",
    "    lr = LinearRegression(regParam=0.0, solver=\"normal\")\n",
    "\n",
    "lr.setMaxIter(5)\n",
    "lr.getMaxIter()\n",
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8895e735-b09f-49b2-a342-29f2405e94f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\n",
      "featuresCol: features column name. (default: features)\n",
      "featuresCols: features column names for multi-column input. (undefined)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 5)\n",
      "num_workers: (cuML) number of Spark CuML workers, where each CuML worker corresponds to one Spark task. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.0)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto, current: normal)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8c766d10-6c69-448a-be07-d43bca4de6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'eig', 'fit_intercept': True, 'normalize': True, 'verbose': False, 'alpha': 0.0, 'solver': 'eig', 'loss': 'squared_loss', 'l1_ratio': 0.0, 'max_iter': 5, 'tol': 1e-06, 'shuffle': True}\n"
     ]
    }
   ],
   "source": [
    "if SPARK_RAPIDS_ML:\n",
    "    print(lr.cuml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "50d3e6b0-14f6-4e9f-85c5-3fab4335f49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.setRegParam(0.1)\n",
    "lr.getRegParam()\n",
    "# 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e35968e1-8af4-4664-994c-83fab9dae385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_c616df9ba5ab"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.setRegParam(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9f8280b3-a086-472d-8e87-7d52d9cd72d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\n",
      "featuresCol: features column name. (default: features)\n",
      "featuresCols: features column names for multi-column input. (undefined)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 5)\n",
      "num_workers: (cuML) number of Spark CuML workers, where each CuML worker corresponds to one Spark task. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.0)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto, current: normal)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8905c16f-5211-4a12-9863-6bdb31ffe54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'eig', 'fit_intercept': True, 'normalize': True, 'verbose': False, 'alpha': 0.0, 'solver': 'eig', 'loss': 'squared_loss', 'l1_ratio': 0.0, 'max_iter': 5, 'tol': 1e-06, 'shuffle': True}\n"
     ]
    }
   ],
   "source": [
    "if SPARK_RAPIDS_ML:\n",
    "    print(lr.cuml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3abe2f15-14b5-43a1-9521-400d3cf56de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/09 14:41:53 WARN TaskSetManager: Lost task 0.0 in stage 60.0 (TID 164) (192.168.86.223 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/leey/dev/leey/spark-cuml/src/spark_rapids_ml/core.py\", line 411, in _train_udf\n",
      "    logger.info(\"Cuml fit complete\")\n",
      "  File \"/home/leey/dev/leey/spark-cuml/src/spark_rapids_ml/regression.py\", line 228, in _linear_regression_fit\n",
      "    linear_regression.fit(\n",
      "  File \"/home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/internals/api_decorators.py\", line 415, in inner\n",
      "    return func(*args, **kwargs)\n",
      "  File \"base_mg.pyx\", line 90, in cuml.linear_model.base_mg.MGFitMixin.fit\n",
      "  File \"/home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/internals/api_decorators.py\", line 415, in inner\n",
      "    return func(*args, **kwargs)\n",
      "  File \"linear_regression_mg.pyx\", line 94, in cuml.linear_model.linear_regression_mg.LinearRegressionMG._fit\n",
      "RuntimeError: exception occured! file=/workspace/.conda-bld/work/cpp/src/glm/ols_mg.cu line=78: olsFit: no algorithm with this id has been implemented\n",
      "Obtained 61 stack frames\n",
      "#0 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN4raft9exception18collect_call_stackEv+0x3b) [0x7f9884b1118b]\n",
      "#1 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN4raft9exceptionC2ENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE+0x61) [0x7f9884b118e1]\n",
      "#2 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN2ML3OLS3opg8fit_implIdEEvRN4raft8handle_tERSt6vectorIPN8MLCommon6Matrix4DataIT_EESaISC_EERNS8_14PartDescriptorESF_PSA_SI_bbiPP11CUstream_stib+0x324) [0x7f988553e314]\n",
      "#3 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN2ML3OLS3opg8fit_implIdEEvRN4raft8handle_tERSt6vectorIPN8MLCommon6Matrix4DataIT_EESaISC_EERNS8_14PartDescriptorESF_PSA_SI_bbib+0x138) [0x7f988553ecc8]\n",
      "#4 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/linear_regression_mg.cpython-39-x86_64-linux-gnu.so(+0x1e1b6) [0x7f983c0641b6]\n",
      "#5 in python3(PyObject_Call+0x157) [0x561eda9c1997]\n",
      "#6 in python3(_PyEval_EvalFrameDefault+0x407d) [0x561eda9a490d]\n",
      "#7 in python3(+0x12a8b7) [0x561eda99f8b7]\n",
      "#8 in python3(+0x14c198) [0x561eda9c1198]\n",
      "#9 in python3(PyVectorcall_Call+0x87) [0x561eda9c1b77]\n",
      "#10 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/base_mg.cpython-39-x86_64-linux-gnu.so(+0x1728f) [0x7f98341c228f]\n",
      "#11 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/base_mg.cpython-39-x86_64-linux-gnu.so(+0x1daca) [0x7f98341c8aca]\n",
      "#12 in python3(PyObject_Call+0x157) [0x561eda9c1997]\n",
      "#13 in python3(_PyEval_EvalFrameDefault+0x407d) [0x561eda9a490d]\n",
      "#14 in python3(+0x12a8b7) [0x561eda99f8b7]\n",
      "#15 in python3(+0x14c0ff) [0x561eda9c10ff]\n",
      "#16 in python3(_PyEval_EvalFrameDefault+0x4c51) [0x561eda9a54e1]\n",
      "#17 in python3(+0x12a8b7) [0x561eda99f8b7]\n",
      "#18 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n",
      "#19 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n",
      "#20 in python3(+0x1538f4) [0x561eda9c88f4]\n",
      "#21 in python3(+0x18ca23) [0x561edaa01a23]\n",
      "#22 in python3(+0x18ca23) [0x561edaa01a23]\n",
      "#23 in python3(_PyEval_EvalFrameDefault+0x932) [0x561eda9a11c2]\n",
      "#24 in python3(+0x1538f4) [0x561eda9c88f4]\n",
      "#25 in python3(_PyEval_EvalFrameDefault+0x932) [0x561eda9a11c2]\n",
      "#26 in python3(+0x1538f4) [0x561eda9c88f4]\n",
      "#27 in python3(_PyEval_EvalFrameDefault+0x932) [0x561eda9a11c2]\n",
      "#28 in python3(+0x13d113) [0x561eda9b2113]\n",
      "#29 in python3(_PyEval_EvalFrameDefault+0x4c51) [0x561eda9a54e1]\n",
      "#30 in python3(+0x12a8b7) [0x561eda99f8b7]\n",
      "#31 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n",
      "#32 in python3(_PyEval_EvalFrameDefault+0x66e) [0x561eda9a0efe]\n",
      "#33 in python3(+0x12a8b7) [0x561eda99f8b7]\n",
      "#34 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n",
      "#35 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n",
      "#36 in python3(+0x12a8b7) [0x561eda99f8b7]\n",
      "#37 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n",
      "#38 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n",
      "#39 in python3(+0x13d113) [0x561eda9b2113]\n",
      "#40 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n",
      "#41 in python3(+0x12a8b7) [0x561eda99f8b7]\n",
      "#42 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n",
      "#43 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n",
      "#44 in python3(+0x12a8b7) [0x561eda99f8b7]\n",
      "#45 in python3(_PyEval_EvalCodeWithName+0x47) [0x561eda99f577]\n",
      "#46 in python3(PyEval_EvalCodeEx+0x39) [0x561eda99f529]\n",
      "#47 in python3(PyEval_EvalCode+0x1b) [0x561edaa5acdb]\n",
      "#48 in python3(+0x1ea76d) [0x561edaa5f76d]\n",
      "#49 in python3(+0x13d79d) [0x561eda9b279d]\n",
      "#50 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n",
      "#51 in python3(+0x12a8b7) [0x561eda99f8b7]\n",
      "#52 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n",
      "#53 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n",
      "#54 in python3(+0x12a8b7) [0x561eda99f8b7]\n",
      "#55 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n",
      "#56 in python3(+0x207a5b) [0x561edaa7ca5b]\n",
      "#57 in python3(Py_RunMain+0xcc) [0x561edaa7bf9c]\n",
      "#58 in python3(Py_BytesMain+0x39) [0x561edaa4e979]\n",
      "#59 in /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x7f99517220b3]\n",
      "#60 in python3(+0x1d9881) [0x561edaa4e881]\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:554)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:507)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:727)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:433)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2079)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:267)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(60, 0) finished unsuccessfully.\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/leey/dev/leey/spark-cuml/src/spark_rapids_ml/core.py\", line 411, in _train_udf\n    logger.info(\"Cuml fit complete\")\n  File \"/home/leey/dev/leey/spark-cuml/src/spark_rapids_ml/regression.py\", line 228, in _linear_regression_fit\n    linear_regression.fit(\n  File \"/home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/internals/api_decorators.py\", line 415, in inner\n    return func(*args, **kwargs)\n  File \"base_mg.pyx\", line 90, in cuml.linear_model.base_mg.MGFitMixin.fit\n  File \"/home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/internals/api_decorators.py\", line 415, in inner\n    return func(*args, **kwargs)\n  File \"linear_regression_mg.pyx\", line 94, in cuml.linear_model.linear_regression_mg.LinearRegressionMG._fit\nRuntimeError: exception occured! file=/workspace/.conda-bld/work/cpp/src/glm/ols_mg.cu line=78: olsFit: no algorithm with this id has been implemented\nObtained 61 stack frames\n#0 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN4raft9exception18collect_call_stackEv+0x3b) [0x7f9884b1118b]\n#1 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN4raft9exceptionC2ENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE+0x61) [0x7f9884b118e1]\n#2 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN2ML3OLS3opg8fit_implIdEEvRN4raft8handle_tERSt6vectorIPN8MLCommon6Matrix4DataIT_EESaISC_EERNS8_14PartDescriptorESF_PSA_SI_bbiPP11CUstream_stib+0x324) [0x7f988553e314]\n#3 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN2ML3OLS3opg8fit_implIdEEvRN4raft8handle_tERSt6vectorIPN8MLCommon6Matrix4DataIT_EESaISC_EERNS8_14PartDescriptorESF_PSA_SI_bbib+0x138) [0x7f988553ecc8]\n#4 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/linear_regression_mg.cpython-39-x86_64-linux-gnu.so(+0x1e1b6) [0x7f983c0641b6]\n#5 in python3(PyObject_Call+0x157) [0x561eda9c1997]\n#6 in python3(_PyEval_EvalFrameDefault+0x407d) [0x561eda9a490d]\n#7 in python3(+0x12a8b7) [0x561eda99f8b7]\n#8 in python3(+0x14c198) [0x561eda9c1198]\n#9 in python3(PyVectorcall_Call+0x87) [0x561eda9c1b77]\n#10 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/base_mg.cpython-39-x86_64-linux-gnu.so(+0x1728f) [0x7f98341c228f]\n#11 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/base_mg.cpython-39-x86_64-linux-gnu.so(+0x1daca) [0x7f98341c8aca]\n#12 in python3(PyObject_Call+0x157) [0x561eda9c1997]\n#13 in python3(_PyEval_EvalFrameDefault+0x407d) [0x561eda9a490d]\n#14 in python3(+0x12a8b7) [0x561eda99f8b7]\n#15 in python3(+0x14c0ff) [0x561eda9c10ff]\n#16 in python3(_PyEval_EvalFrameDefault+0x4c51) [0x561eda9a54e1]\n#17 in python3(+0x12a8b7) [0x561eda99f8b7]\n#18 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n#19 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n#20 in python3(+0x1538f4) [0x561eda9c88f4]\n#21 in python3(+0x18ca23) [0x561edaa01a23]\n#22 in python3(+0x18ca23) [0x561edaa01a23]\n#23 in python3(_PyEval_EvalFrameDefault+0x932) [0x561eda9a11c2]\n#24 in python3(+0x1538f4) [0x561eda9c88f4]\n#25 in python3(_PyEval_EvalFrameDefault+0x932) [0x561eda9a11c2]\n#26 in python3(+0x1538f4) [0x561eda9c88f4]\n#27 in python3(_PyEval_EvalFrameDefault+0x932) [0x561eda9a11c2]\n#28 in python3(+0x13d113) [0x561eda9b2113]\n#29 in python3(_PyEval_EvalFrameDefault+0x4c51) [0x561eda9a54e1]\n#30 in python3(+0x12a8b7) [0x561eda99f8b7]\n#31 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n#32 in python3(_PyEval_EvalFrameDefault+0x66e) [0x561eda9a0efe]\n#33 in python3(+0x12a8b7) [0x561eda99f8b7]\n#34 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n#35 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n#36 in python3(+0x12a8b7) [0x561eda99f8b7]\n#37 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n#38 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n#39 in python3(+0x13d113) [0x561eda9b2113]\n#40 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n#41 in python3(+0x12a8b7) [0x561eda99f8b7]\n#42 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n#43 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n#44 in python3(+0x12a8b7) [0x561eda99f8b7]\n#45 in python3(_PyEval_EvalCodeWithName+0x47) [0x561eda99f577]\n#46 in python3(PyEval_EvalCodeEx+0x39) [0x561eda99f529]\n#47 in python3(PyEval_EvalCode+0x1b) [0x561edaa5acdb]\n#48 in python3(+0x1ea76d) [0x561edaa5f76d]\n#49 in python3(+0x13d79d) [0x561eda9b279d]\n#50 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n#51 in python3(+0x12a8b7) [0x561eda99f8b7]\n#52 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n#53 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n#54 in python3(+0x12a8b7) [0x561eda99f8b7]\n#55 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n#56 in python3(+0x207a5b) [0x561edaa7ca5b]\n#57 in python3(Py_RunMain+0xcc) [0x561edaa7bf9c]\n#58 in python3(Py_BytesMain+0x39) [0x561edaa4e979]\n#59 in /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x7f99517220b3]\n#60 in python3(+0x1d9881) [0x561edaa4e881]\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:554)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:507)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:727)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:433)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2079)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:267)\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2789)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2725)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2724)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2724)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2162)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2916)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2262)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2283)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2302)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2327)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# RuntimeError: exception occured! file=/workspace/.conda-bld/work/cpp/src/glm/ols_mg.cu line=78: olsFit: no algorithm with this id has been implemented\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/leey/spark-cuml/src/spark_rapids_ml/core.py:418\u001b[0m, in \u001b[0;36m_CumlEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mpartitionId() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mresult)\n\u001b[1;32m    417\u001b[0m ret \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 418\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapInPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_train_udf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_out_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    419\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbarrier\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m )\n\u001b[1;32m    424\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_pyspark_model(ret))\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_cuml_params(model)\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/pyspark/rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1813\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1814\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/pyspark/errors/exceptions.py:219\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    221\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/devpub/leewyang/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(60, 0) finished unsuccessfully.\norg.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/leey/dev/leey/spark-cuml/src/spark_rapids_ml/core.py\", line 411, in _train_udf\n    logger.info(\"Cuml fit complete\")\n  File \"/home/leey/dev/leey/spark-cuml/src/spark_rapids_ml/regression.py\", line 228, in _linear_regression_fit\n    linear_regression.fit(\n  File \"/home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/internals/api_decorators.py\", line 415, in inner\n    return func(*args, **kwargs)\n  File \"base_mg.pyx\", line 90, in cuml.linear_model.base_mg.MGFitMixin.fit\n  File \"/home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/internals/api_decorators.py\", line 415, in inner\n    return func(*args, **kwargs)\n  File \"linear_regression_mg.pyx\", line 94, in cuml.linear_model.linear_regression_mg.LinearRegressionMG._fit\nRuntimeError: exception occured! file=/workspace/.conda-bld/work/cpp/src/glm/ols_mg.cu line=78: olsFit: no algorithm with this id has been implemented\nObtained 61 stack frames\n#0 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN4raft9exception18collect_call_stackEv+0x3b) [0x7f9884b1118b]\n#1 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN4raft9exceptionC2ENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE+0x61) [0x7f9884b118e1]\n#2 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN2ML3OLS3opg8fit_implIdEEvRN4raft8handle_tERSt6vectorIPN8MLCommon6Matrix4DataIT_EESaISC_EERNS8_14PartDescriptorESF_PSA_SI_bbiPP11CUstream_stib+0x324) [0x7f988553e314]\n#3 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/common/../../../../libcuml++.so(_ZN2ML3OLS3opg8fit_implIdEEvRN4raft8handle_tERSt6vectorIPN8MLCommon6Matrix4DataIT_EESaISC_EERNS8_14PartDescriptorESF_PSA_SI_bbib+0x138) [0x7f988553ecc8]\n#4 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/linear_regression_mg.cpython-39-x86_64-linux-gnu.so(+0x1e1b6) [0x7f983c0641b6]\n#5 in python3(PyObject_Call+0x157) [0x561eda9c1997]\n#6 in python3(_PyEval_EvalFrameDefault+0x407d) [0x561eda9a490d]\n#7 in python3(+0x12a8b7) [0x561eda99f8b7]\n#8 in python3(+0x14c198) [0x561eda9c1198]\n#9 in python3(PyVectorcall_Call+0x87) [0x561eda9c1b77]\n#10 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/base_mg.cpython-39-x86_64-linux-gnu.so(+0x1728f) [0x7f98341c228f]\n#11 in /home/leey/miniconda3/envs/rapids-22.10/lib/python3.9/site-packages/cuml/linear_model/base_mg.cpython-39-x86_64-linux-gnu.so(+0x1daca) [0x7f98341c8aca]\n#12 in python3(PyObject_Call+0x157) [0x561eda9c1997]\n#13 in python3(_PyEval_EvalFrameDefault+0x407d) [0x561eda9a490d]\n#14 in python3(+0x12a8b7) [0x561eda99f8b7]\n#15 in python3(+0x14c0ff) [0x561eda9c10ff]\n#16 in python3(_PyEval_EvalFrameDefault+0x4c51) [0x561eda9a54e1]\n#17 in python3(+0x12a8b7) [0x561eda99f8b7]\n#18 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n#19 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n#20 in python3(+0x1538f4) [0x561eda9c88f4]\n#21 in python3(+0x18ca23) [0x561edaa01a23]\n#22 in python3(+0x18ca23) [0x561edaa01a23]\n#23 in python3(_PyEval_EvalFrameDefault+0x932) [0x561eda9a11c2]\n#24 in python3(+0x1538f4) [0x561eda9c88f4]\n#25 in python3(_PyEval_EvalFrameDefault+0x932) [0x561eda9a11c2]\n#26 in python3(+0x1538f4) [0x561eda9c88f4]\n#27 in python3(_PyEval_EvalFrameDefault+0x932) [0x561eda9a11c2]\n#28 in python3(+0x13d113) [0x561eda9b2113]\n#29 in python3(_PyEval_EvalFrameDefault+0x4c51) [0x561eda9a54e1]\n#30 in python3(+0x12a8b7) [0x561eda99f8b7]\n#31 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n#32 in python3(_PyEval_EvalFrameDefault+0x66e) [0x561eda9a0efe]\n#33 in python3(+0x12a8b7) [0x561eda99f8b7]\n#34 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n#35 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n#36 in python3(+0x12a8b7) [0x561eda99f8b7]\n#37 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n#38 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n#39 in python3(+0x13d113) [0x561eda9b2113]\n#40 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n#41 in python3(+0x12a8b7) [0x561eda99f8b7]\n#42 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n#43 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n#44 in python3(+0x12a8b7) [0x561eda99f8b7]\n#45 in python3(_PyEval_EvalCodeWithName+0x47) [0x561eda99f577]\n#46 in python3(PyEval_EvalCodeEx+0x39) [0x561eda99f529]\n#47 in python3(PyEval_EvalCode+0x1b) [0x561edaa5acdb]\n#48 in python3(+0x1ea76d) [0x561edaa5f76d]\n#49 in python3(+0x13d79d) [0x561eda9b279d]\n#50 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n#51 in python3(+0x12a8b7) [0x561eda99f8b7]\n#52 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n#53 in python3(_PyEval_EvalFrameDefault+0x3bf) [0x561eda9a0c4f]\n#54 in python3(+0x12a8b7) [0x561eda99f8b7]\n#55 in python3(_PyFunction_Vectorcall+0xb9) [0x561eda9b1e09]\n#56 in python3(+0x207a5b) [0x561edaa7ca5b]\n#57 in python3(Py_RunMain+0xcc) [0x561edaa7bf9c]\n#58 in python3(Py_BytesMain+0x39) [0x561edaa4e979]\n#59 in /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf3) [0x7f99517220b3]\n#60 in python3(+0x1d9881) [0x561edaa4e881]\n\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:554)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:507)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:727)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:433)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2079)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:267)\n\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2789)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2725)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2724)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2724)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2162)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2916)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2262)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2283)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2302)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2327)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "# RuntimeError: exception occured! file=/workspace/.conda-bld/work/cpp/src/glm/ols_mg.cu line=78: olsFit: no algorithm with this id has been implemented\n",
    "model = lr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ecfce-0314-435a-b480-f6b5a930226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.setFeaturesCol(\"features\")\n",
    "model.setPredictionCol(\"newPrediction\")\n",
    "model.getMaxIter()\n",
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6db09-c0c6-4651-903c-295adc035612",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.getMaxBlockSizeInMB()\n",
    "# 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f699f7-41f2-4558-8e09-f76c4aa49bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test0 = spark.createDataFrame([(Vectors.dense(1.0),)], [\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b3930-9f59-442b-b0ed-2a429724e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(model.predict(test0.head().features) - (-1.0)) < 0.001\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf88ae4-671f-4ec1-9fa7-31a4d84cd46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(model.transform(test0).head().newPrediction - (-1.0)) < 0.001\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b88b68-904e-471f-8fb3-692c779d5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(model.coefficients[0] - 1.0) < 0.001\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a77feec-cbb1-4680-a365-528645c4b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(model.intercept - 0.0) < 0.001\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b792e-9601-4de6-a114-8d329fc9f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = spark.createDataFrame([(Vectors.sparse(1, [0], [1.0]),)], [\"features\"])\n",
    "abs(model.transform(test1).head().newPrediction - 1.0) < 0.001\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b59367-2302-446c-8348-e2b510c2a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.setParams(featuresCol=\"vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e07233-807e-479a-883c-f9097dcd592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"/tmp\"\n",
    "lr_path = temp_path + \"/lr\"\n",
    "shutil.rmtree(lr_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9631c094-5321-4d41-acdc-b2752979d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.save(lr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4dc2f-e09b-47f3-a0a3-abd3d9e174b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LinearRegression.load(lr_path)\n",
    "lr2.getMaxIter()\n",
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9e46c-0c88-4c0c-9f8e-ac0ad457ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = temp_path + \"/lr_model\"\n",
    "shutil.rmtree(model_path, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80def2f-34d2-427e-826f-59c6e7c906e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3853568-f176-42d0-afad-25f69756ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LinearRegressionModel.load(model_path)\n",
    "model.coefficients[0] == model2.coefficients[0]\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf13eb1-7be8-443c-89d3-955be5044e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept == model2.intercept\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2f3e4a-fcaf-4390-90d3-55e0d9b0f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transform(test0).take(1) == model2.transform(test0).take(1)\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ac2c7-6f7e-4eef-95fc-25cf684afa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.numFeatures\n",
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c82d92-8826-4e1a-bf8b-295cf4efcd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(model_path + \"_2\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613cd8d-4234-46f7-9c85-04e6ccd8c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().format(\"pmml\").save(model_path + \"_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b027a046-1d81-4afe-b2f0-d5a746a4afbd",
   "metadata": {},
   "source": [
    "## LinearRegression (custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4cf7b91a-b2b7-4bf3-b41f-dae23a1f173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYSPARK = False\n",
    "SPARK_RAPIDS_ML = not PYSPARK\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "15e16933-0673-41b6-ab23-a85e8951042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import array, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "630856e5-1431-4cd7-bc03-9f1fb2a0d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n",
    "else:\n",
    "    from spark_rapids_ml.regression import LinearRegression, LinearRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7130d47c-f6b4-436d-ab72-f0d82f29caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(\n",
    "    [[-0.20515826,  1.4940791 ],\n",
    "     [ 0.12167501,  0.7610377 ],\n",
    "     [ 1.4542735,   0.14404356],\n",
    "     [-0.85409576,  0.3130677 ],\n",
    "     [ 2.2408931,   0.978738  ],\n",
    "     [-0.1513572,   0.95008844],\n",
    "     [-0.9772779,   1.867558  ],\n",
    "     [ 0.41059852, -0.10321885]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0335f18f-d139-41ca-b094-047e40a5e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([2.0374513, 22.403986, 139.4456, -76.19584, 225.72075, -0.6784152, -65.54835, 37.30829])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "30c9d584-66c2-43a9-9cd4-aafad4bb2e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"c0\", \"c1\"]\n",
    "label_col = \"label_col\"\n",
    "schema = [\"c0 float, c1 float, label_col float\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "11f4a34c-cc6c-440d-b289-e59fa2c5a57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['c0', 'c1'], 'label_col')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols, label_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "29e50450-1869-496e-a5ff-b6078c85b454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c0 float, c1 float, label_col float']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2ec95d8d-30e0-4faf-b5ff-9516a084c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    np.concatenate((X, y.reshape(8, 1)), axis=1).tolist(),\n",
    "    \",\".join(schema),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9d049829-6e03-485c-8b24-6aba05ac5834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+\n",
      "|         c0|         c1| label_col|\n",
      "+-----------+-----------+----------+\n",
      "|-0.20515826|  1.4940791| 2.0374513|\n",
      "| 0.12167501|  0.7610377| 22.403986|\n",
      "|  1.4542735| 0.14404356|  139.4456|\n",
      "|-0.85409576|  0.3130677| -76.19584|\n",
      "|  2.2408931|   0.978738| 225.72075|\n",
      "| -0.1513572| 0.95008844|-0.6784152|\n",
      "| -0.9772779|   1.867558| -65.54835|\n",
      "| 0.41059852|-0.10321885|  37.30829|\n",
      "+-----------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c198fe13-d403-403c-8be7-cca8c1e0491e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "| label_col|            features|\n",
      "+----------+--------------------+\n",
      "| 2.0374513|[-0.20515826, 1.4...|\n",
      "| 22.403986|[0.12167501, 0.76...|\n",
      "|  139.4456|[1.4542735, 0.144...|\n",
      "| -76.19584|[-0.85409576, 0.3...|\n",
      "| 225.72075|[2.2408931, 0.978...|\n",
      "|-0.6784152|[-0.1513572, 0.95...|\n",
      "| -65.54835|[-0.9772779, 1.86...|\n",
      "|  37.30829|[0.41059852, -0.1...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('label_col', FloatType(), True), StructField('features', ArrayType(FloatType(), True), False)])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn(\"features\", array(*feature_cols)).drop(*feature_cols)\n",
    "df.show(); df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bb9dddbd-cdac-4d15-84d1-f006a97454e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYSPARK:\n",
    "    # requires VectorUDT\n",
    "    df = df.withColumn(\"features_vec\", array_to_vector(\"features\")).drop(\"features\").withColumnRenamed(\"features_vec\", \"features\")\n",
    "    df.show()\n",
    "    print(df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "72811822-4182-4403-95b1-6bf643d1b7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Spark Param 'aggregationDepth' is not used by CuML.\n",
      "WARNING: Spark Param 'epsilon' is not used by CuML.\n",
      "WARNING: Spark Param 'maxBlockSizeInMB' is not used by CuML.\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ebde896d-8b40-4ba4-ae98-1af2e1a9f787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression_a13e102e88d6"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.setFeaturesCol(\"features\")\n",
    "lr.setRegParam(0.0)\n",
    "lr.setLabelCol(\"label_col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "64dee2e1-91ee-490e-88cd-235f4e625361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "featuresCols: features column names for multi-column input. (undefined)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: label_col)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "num_workers: (cuML) number of Spark CuML workers, where each CuML worker corresponds to one Spark task. (default: 1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.0)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "68a11804-77ad-4d44-a6bd-d3912dc835c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'eig', 'fit_intercept': True, 'normalize': True, 'verbose': False, 'alpha': 0.0, 'solver': 'eig', 'loss': 'squared_loss', 'l1_ratio': 0.0, 'max_iter': 100, 'tol': 1e-06, 'shuffle': True}\n"
     ]
    }
   ],
   "source": [
    "if SPARK_RAPIDS_ML:\n",
    "    print(lr.cuml_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "694314a4-a707-4b1b-86f5-ce645ae881bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Spark Param 'aggregationDepth' is not used by CuML.\n",
      "WARNING: Spark Param 'epsilon' is not used by CuML.\n",
      "WARNING: Spark Param 'maxBlockSizeInMB' is not used by CuML.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9bd54034-cd9d-486a-ab21-d5819d7264a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[94.46691131591797, 14.33534049987793]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.coefficients\n",
    "# [94.46689350900762,14.33532962562045]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8dffc-44a2-4cce-9bed-31fd228d146b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
